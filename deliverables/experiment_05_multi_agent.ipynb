{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 5: Multi-Agent System"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Setup\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional, Tuple\nfrom dataclasses import dataclass, asdict, field\nfrom enum import Enum\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# RAG\nimport chromadb\nfrom chromadb.utils import embedding_functions\n\n# LLM\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nsys.path.append('..')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nprint(\"Imports loaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "DB_PATH = Path(\"../data/vector_db\")\n",
    "MODEL_PATH = Path(\"/home/sskaplun/study/genAI/kaggle/models/gemma-2-9b-it\")\n",
    "OUTPUT_DIR = Path(\"../evaluation/experiment_05\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "COLLECTION_NAME = \"ukrainian_math\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# Multi-agent parameters\n",
    "TOP_K = 5\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 512\n",
    "MAX_ITERATIONS = 2  # Max refinement iterations\n",
    "QUALITY_THRESHOLD = 0.7  # Validation threshold\n",
    "\n",
    "print(f\"Max Iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"Quality Threshold: {QUALITY_THRESHOLD}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class AgentRole(Enum):\n    TOPIC = \"topic_agent\"\n    TASK_GENERATOR = \"task_generator\"\n    SOLUTION = \"solution_agent\"\n    QUALITY = \"quality_agent\"\n    ORCHESTRATOR = \"orchestrator\"\n\n@dataclass\nclass AgentMessage:\n    role: AgentRole\n    content: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n@dataclass\nclass MultiAgentResponse:\n    question: str\n    final_answer: str\n    task_text: str\n    solution_text: str\n    conversation_history: List[AgentMessage]\n    citations: List[str]\n    avg_relevance: float  # NEW: retrieval quality from TopicAgent\n    iterations: int\n    quality_score: float\n    answer_length: int\n    \n    def to_dict(self):\n        return {\n            'question': self.question,\n            'final_answer': self.final_answer,\n            'task_text': self.task_text,\n            'solution_text': self.solution_text,\n            'avg_relevance': self.avg_relevance,\n            'iterations': self.iterations,\n            'quality_score': self.quality_score,\n            'answer_length': self.answer_length,\n            'num_messages': len(self.conversation_history)\n        }\n\nprint(\"Dataclasses defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"LOADING INFRASTRUCTURE\")\nprint(\"=\"*80)\n\n# Vector DB\nclient = chromadb.PersistentClient(path=str(DB_PATH))\nembedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n    model_name=EMBEDDING_MODEL\n)\ncollection = client.get_collection(\n    name=COLLECTION_NAME,\n    embedding_function=embedding_function\n)\nprint(f\"Vector DB: {collection.count():,} chunks\")\n\n# LLM\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\nmodel = AutoModelForCausalLM.from_pretrained(\n    str(MODEL_PATH),\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\nprint(\"LLM loaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def generate_llm(prompt: str, max_tokens: int = MAX_NEW_TOKENS, temp: float = TEMPERATURE) -> str:\n    \"\"\"Core LLM generation function used by all agents.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    formatted = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_tokens,\n            temperature=temp,\n            top_p=0.9,\n            do_sample=temp > 0,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    return tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n\nprint(\"Core LLM function defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class TopicAgent:\n    \"\"\"Agent 1: Retrieves relevant textbook context for a topic.\"\"\"\n    \n    def __init__(self, collection, k: int = TOP_K):\n        self.collection = collection\n        self.k = k\n        self.role = AgentRole.TOPIC\n    \n    def retrieve(self, topic: str) -> AgentMessage:\n        \"\"\"Retrieve context for topic.\"\"\"\n        results = self.collection.query(\n            query_texts=[topic],\n            n_results=self.k\n        )\n        \n        chunks = []\n        citations = []\n        \n        for i, (doc, meta, dist) in enumerate(zip(\n            results['documents'][0],\n            results['metadatas'][0],\n            results['distances'][0]\n        ), 1):\n            citation = f\"[{meta['filename']}, с. {meta['page_start']}-{meta['page_end']}]\"\n            header = f\"[Джерело {i}] {citation} | Тип: {meta['content_type']}\"\n            chunks.append(f\"{header}\\n{doc}\")\n            citations.append(citation)\n        \n        context = \"\\n\\n\".join(chunks)\n        avg_relevance = float(np.mean([1 - d for d in results['distances'][0]]))\n        \n        return AgentMessage(\n            role=self.role,\n            content=context,\n            metadata={\n                'citations': citations,\n                'avg_relevance': avg_relevance,\n                'num_chunks': len(chunks)\n            }\n        )\n\nprint(\"TopicAgent defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class TaskGeneratorAgent:\n    \"\"\"Agent 2: Generates math problems from context.\"\"\"\n    \n    def __init__(self):\n        self.role = AgentRole.TASK_GENERATOR\n        self.system_prompt = \"\"\"Ти — експерт-агент з ГЕНЕРАЦІЇ математичних задач українською мовою.\n\nТвоя ЄДИНА задача: згенерувати ТІЛЬКИ текст задачі (умову) на основі контексту.\n\nПравила:\n- Використовуй ТІЛЬКИ інформацію з наданого контексту\n- Формулюй задачу чітко та зрозуміло\n- Включи конкретні числові значення\n- Використовуй українську математичну термінологію\n- НЕ пиши розв'язання (це зробить інший агент)\n\nФормат:\n**Задача:** [текст умови задачі]\n\nВсе! Більше нічого не пиши.\"\"\"\n    \n    def generate(self, context: str, topic: str) -> AgentMessage:\n        \"\"\"Generate task from context.\"\"\"\n        prompt = f\"{self.system_prompt}\\n\\nКОНТЕКСТ:\\n{context}\\n\\nТЕМА: {topic}\\n\\nТВОЯ ЗАДАЧА:\"\n        task = generate_llm(prompt, max_tokens=300)\n        \n        return AgentMessage(\n            role=self.role,\n            content=task,\n            metadata={'topic': topic}\n        )\n\nprint(\"TaskGeneratorAgent defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class SolutionAgent:\n    \"\"\"Agent 3: Solves math problems step-by-step.\"\"\"\n    \n    def __init__(self):\n        self.role = AgentRole.SOLUTION\n        self.system_prompt = \"\"\"Ти — експерт-агент з РОЗВ'ЯЗУВАННЯ математичних задач українською мовою.\n\nТвоя ЄДИНА задача: надати покрокове розв'язання для заданої задачі.\n\nПравила:\n- Розв'язуй крок за кроком\n- Поясни кожен крок зрозумілою мовою\n- Використовуй формули з контексту\n- Перевіряй обчислення\n- Дай фінальну відповідь\n\nФормат:\n**Розв'язання:**\n1. [перший крок з поясненням]\n2. [другий крок]\n...\n\n**Відповідь:** [фінальна відповідь]\"\"\"\n    \n    def solve(self, task: str, context: str) -> AgentMessage:\n        \"\"\"Solve the task.\"\"\"\n        prompt = f\"{self.system_prompt}\\n\\nКОНТЕКСТ (формули та теорія):\\n{context}\\n\\n{task}\\n\\nТВОЄ РОЗВ'ЯЗАННЯ:\"\n        solution = generate_llm(prompt, max_tokens=500)\n        \n        return AgentMessage(\n            role=self.role,\n            content=solution,\n            metadata={'task': task}\n        )\n\nprint(\"SolutionAgent defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class QualityAgent:\n    \"\"\"Agent 4: Validates task and solution quality.\"\"\"\n    \n    def __init__(self):\n        self.role = AgentRole.QUALITY\n        self.system_prompt = \"\"\"Ти — експерт-агент з КОНТРОЛЮ ЯКОСТІ математичних задач та розв'язань.\n\nТвоя задача: оцінити якість задачі та розв'язання за критеріями:\n1. Чіткість формулювання задачі (0-1)\n2. Коректність розв'язання (0-1)\n3. Повнота пояснення (0-1)\n4. Українська мова (0-1)\n5. Відповідність контексту (0-1)\n\nФормат відповіді:\nОЦІНКА: [число від 0.0 до 1.0]\nКОМЕНТАР: [короткий коментар]\nПРОПОЗИЦІЇ: [що покращити, якщо оцінка < 0.7]\"\"\"\n    \n    def validate(self, task: str, solution: str, context: str) -> Tuple[float, str, AgentMessage]:\n        \"\"\"Validate quality and return score, feedback, and message.\"\"\"\n        prompt = f\"{self.system_prompt}\\n\\nКОНТЕКСТ:\\n{context}\\n\\n{task}\\n\\n{solution}\\n\\nТВОЯ ОЦІНКА:\"\n        feedback = generate_llm(prompt, max_tokens=200, temp=0.3)\n        \n        # Extract score (simple regex)\n        import re\n        score_match = re.search(r'ОЦІНКА:\\s*([0-9.]+)', feedback)\n        score = float(score_match.group(1)) if score_match else 0.5\n        score = max(0.0, min(1.0, score))  # Clamp to [0, 1]\n        \n        return score, feedback, AgentMessage(\n            role=self.role,\n            content=feedback,\n            metadata={'score': score}\n        )\n\nprint(\"QualityAgent defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class Orchestrator:\n    \"\"\"Agent 5: Coordinates multi-agent collaboration.\"\"\"\n    \n    def __init__(\n        self,\n        topic_agent: TopicAgent,\n        task_agent: TaskGeneratorAgent,\n        solution_agent: SolutionAgent,\n        quality_agent: QualityAgent,\n        quality_threshold: float = QUALITY_THRESHOLD,\n        max_iterations: int = MAX_ITERATIONS\n    ):\n        self.topic_agent = topic_agent\n        self.task_agent = task_agent\n        self.solution_agent = solution_agent\n        self.quality_agent = quality_agent\n        self.quality_threshold = quality_threshold\n        self.max_iterations = max_iterations\n        self.role = AgentRole.ORCHESTRATOR\n    \n    def run(self, question: str, verbose: bool = False) -> MultiAgentResponse:\n        \"\"\"Orchestrate multi-agent workflow.\"\"\"\n        conversation = []\n        \n        if verbose:\n            print(f\"\\n[ORCHESTRATOR] Starting workflow for: {question}\")\n            print(\"-\" * 80)\n        \n        # Step 1: Topic Agent retrieves context\n        if verbose:\n            print(\"\\n[1] TopicAgent: Retrieving context...\")\n        \n        topic_msg = self.topic_agent.retrieve(question)\n        conversation.append(topic_msg)\n        \n        if verbose:\n            print(f\"    Retrieved {topic_msg.metadata['num_chunks']} chunks\")\n            print(f\"    Avg relevance: {topic_msg.metadata['avg_relevance']:.3f}\")\n        \n        context = topic_msg.content\n        citations = topic_msg.metadata['citations']\n        avg_relevance = topic_msg.metadata['avg_relevance']  # NEW: capture retrieval quality\n        \n        # Iterative refinement loop\n        for iteration in range(1, self.max_iterations + 1):\n            if verbose:\n                print(f\"\\n[ITERATION {iteration}]\")\n            \n            # Step 2: Task Generator creates problem\n            if verbose:\n                print(\"  [2] TaskGeneratorAgent: Creating task...\")\n            \n            task_msg = self.task_agent.generate(context, question)\n            conversation.append(task_msg)\n            task_text = task_msg.content\n            \n            if verbose:\n                print(f\"      Task: {task_text[:80]}...\")\n            \n            # Step 3: Solution Agent solves\n            if verbose:\n                print(\"  [3] SolutionAgent: Solving task...\")\n            \n            solution_msg = self.solution_agent.solve(task_text, context)\n            conversation.append(solution_msg)\n            solution_text = solution_msg.content\n            \n            if verbose:\n                print(f\"      Solution: {solution_text[:80]}...\")\n            \n            # Step 4: Quality Agent validates\n            if verbose:\n                print(\"  [4] QualityAgent: Validating...\")\n            \n            score, feedback, quality_msg = self.quality_agent.validate(\n                task_text, solution_text, context\n            )\n            conversation.append(quality_msg)\n            \n            if verbose:\n                print(f\"      Quality Score: {score:.3f}\")\n                print(f\"      Threshold: {self.quality_threshold}\")\n            \n            # Check if quality is acceptable\n            if score >= self.quality_threshold:\n                if verbose:\n                    print(f\"\\n[ORCHESTRATOR] Quality acceptable. Completing workflow.\")\n                break\n            elif iteration < self.max_iterations:\n                if verbose:\n                    print(f\"\\n[ORCHESTRATOR] Quality below threshold. Refining...\")\n                # In a real system, we'd use feedback to guide refinement\n                # For simplicity, we just retry\n            else:\n                if verbose:\n                    print(f\"\\n[ORCHESTRATOR] Max iterations reached. Accepting current result.\")\n        \n        # Combine final answer\n        final_answer = f\"{task_text}\\n\\n{solution_text}\"\n        \n        return MultiAgentResponse(\n            question=question,\n            final_answer=final_answer,\n            task_text=task_text,\n            solution_text=solution_text,\n            conversation_history=conversation,\n            citations=citations,\n            avg_relevance=avg_relevance,  # NEW: pass retrieval quality\n            iterations=iteration,\n            quality_score=score,\n            answer_length=len(final_answer)\n        )\n\nprint(\"Orchestrator defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"INITIALIZING MULTI-AGENT SYSTEM\")\nprint(\"=\"*80)\n\n# Create agents\ntopic_agent = TopicAgent(collection, k=TOP_K)\ntask_agent = TaskGeneratorAgent()\nsolution_agent = SolutionAgent()\nquality_agent = QualityAgent()\n\n# Create orchestrator\norchestrator = Orchestrator(\n    topic_agent=topic_agent,\n    task_agent=task_agent,\n    solution_agent=solution_agent,\n    quality_agent=quality_agent,\n    quality_threshold=QUALITY_THRESHOLD,\n    max_iterations=MAX_ITERATIONS\n)\n\nprint(\"Agents initialized:\")\nprint(\"  - TopicAgent (RAG)\")\nprint(\"  - TaskGeneratorAgent\")\nprint(\"  - SolutionAgent\")\nprint(\"  - QualityAgent\")\nprint(\"  - Orchestrator\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from common import STANDARD_TEST_QUESTIONS, EVALUATION_DATASET\n\nTEST_QUESTIONS = STANDARD_TEST_QUESTIONS[:5]  # Use first 5 questions for multi-agent\nprint(f\"Test set: {len(TEST_QUESTIONS)} questions\")\n\n# Create mapping of questions to expected answers\nquestion_to_expected = {q['input']: q['expected_answer'] for q in EVALUATION_DATASET}\nprint(f\"Expected answers loaded for {len(question_to_expected)} questions\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Multi-Agent Experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"RUNNING MULTI-AGENT EXPERIMENT\")\nprint(\"=\"*80)\n\nresponses = []\n\nfor i, question in enumerate(TEST_QUESTIONS, 1):\n    print(f\"\\n{'='*80}\")\n    print(f\"QUESTION {i}/{len(TEST_QUESTIONS)}: {question}\")\n    print(\"=\"*80)\n    \n    response = orchestrator.run(question, verbose=True)\n    responses.append(response)\n    \n    print(f\"\\n[FINAL RESULT]\")\n    print(\"-\"*80)\n    print(response.final_answer)\n    print(\"-\"*80)\n    print(f\"Iterations: {response.iterations}\")\n    print(f\"Quality Score: {response.quality_score:.3f}\")\n    print(f\"Citations: {len(response.citations)}\")\n\nprint(f\"\\n{'='*80}\")\nprint(f\"Completed {len(responses)} multi-agent workflows\")\nprint(\"=\"*80)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import common\n\nprint(\"Evaluation functions loaded from common.py\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate\nprint(\"=\"*80)\nprint(\"EVALUATION\")\nprint(\"=\"*80)\n\nevaluations = []\n\nfor i, response in enumerate(responses, 1):\n    expected_answer = question_to_expected.get(response.question, None)\n    metrics = common.evaluate_multi_agent(\n        response.final_answer, \n        response.answer_length, \n        response.avg_relevance, \n        response.quality_score, \n        response.iterations,\n        expected_answer\n    )\n    evaluations.append({\n        'question': response.question,\n        'metrics': metrics,\n        'answer_length': response.answer_length,\n        'iterations': response.iterations,\n        'quality_score': response.quality_score\n    })\n    \n    print(f\"\\n{i}. {response.question[:50]}...\")\n    print(f\"   Overall: {metrics['overall_score']:.3f} | \"\n          f\"Quality: {metrics['quality_score']:.3f} | \"\n          f\"Iterations: {response.iterations}\")\n\n# Summary\nprint(f\"\\n{'='*80}\")\nprint(\"SUMMARY\")\nprint(\"=\"*80)\n\navg_metrics = {\n    'overall_score': np.mean([e['metrics']['overall_score'] for e in evaluations]),\n    'quality_score': np.mean([e['metrics']['quality_score'] for e in evaluations]),\n    'retrieval_quality': np.mean([e['metrics']['retrieval_quality'] for e in evaluations]),\n    'ukrainian_ratio': np.mean([e['metrics']['ukrainian_ratio'] for e in evaluations]),\n    'completeness': np.mean([e['metrics']['completeness'] for e in evaluations]),\n    'correctness': np.mean([e['metrics']['correctness'] for e in evaluations]),\n    'structure_rate': sum(e['metrics']['has_structure'] for e in evaluations) / len(evaluations),\n    'citation_rate': sum(e['metrics']['has_citations'] for e in evaluations) / len(evaluations),\n    'collaboration_quality': np.mean([e['metrics']['collaboration_quality'] for e in evaluations]),\n    'avg_iterations': np.mean([e['iterations'] for e in evaluations])\n}\n\nfor key, value in avg_metrics.items():\n    print(f\"  {key:25s}: {value:.3f}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = {\n    'experiment': 'multi_agent_system',\n    'description': 'Specialized agents with orchestration and quality validation',\n    'architecture': {\n        'agents': [\n            'TopicAgent (RAG)',\n            'TaskGeneratorAgent',\n            'SolutionAgent',\n            'QualityAgent',\n            'Orchestrator'\n        ],\n        'workflow': 'Retrieve → Generate Task → Solve → Validate → Iterate if needed',\n        'max_iterations': MAX_ITERATIONS,\n        'quality_threshold': QUALITY_THRESHOLD\n    },\n    'avg_metrics': avg_metrics,\n    'responses': [r.to_dict() for r in responses],\n    'evaluations': evaluations\n}\n\nwith open(OUTPUT_DIR / 'results.json', 'w', encoding='utf-8') as f:\n    json.dump(results, f, ensure_ascii=False, indent=2)\n\nprint(f\"Results saved to {OUTPUT_DIR}\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPERIMENT 5 COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\\nOverall Score: {avg_metrics['overall_score']:.3f}\")\nprint(f\"Quality Score: {avg_metrics['quality_score']:.3f}\")\nprint(f\"Avg Iterations: {avg_metrics['avg_iterations']:.1f}\")\nprint(f\"Collaboration Quality: {avg_metrics['collaboration_quality']:.3f}\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
