{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 3: Advanced RAG"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Setup\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Set, Tuple\nfrom dataclasses import dataclass, asdict\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# RAG components\nimport chromadb\nfrom chromadb.utils import embedding_functions\n\n# LLM\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nsys.path.append('..')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nprint(\"Imports loaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "DB_PATH = Path(\"../data/vector_db\")\n",
    "MODEL_PATH = Path(\"/home/sskaplun/study/genAI/kaggle/models/gemma-2-9b-it\")\n",
    "OUTPUT_DIR = Path(\"../evaluation/experiment_03\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "COLLECTION_NAME = \"ukrainian_math\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# Advanced RAG parameters\n",
    "NUM_QUERY_EXPANSIONS = 3  # Generate 3 query variants\n",
    "RETRIEVAL_K = 15  # Retrieve more candidates\n",
    "FINAL_K = 5  # Re-rank to top-5\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "# Re-ranking weights\n",
    "RELEVANCE_WEIGHT = 0.5\n",
    "DIVERSITY_WEIGHT = 0.3\n",
    "CONTENT_TYPE_WEIGHT = 0.2\n",
    "\n",
    "print(f\"Query Expansions: {NUM_QUERY_EXPANSIONS}\")\n",
    "print(f\"Retrieval K: {RETRIEVAL_K} → Final K: {FINAL_K}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "@dataclass\nclass RetrievedChunk:\n    text: str\n    content_type: str\n    confidence: float\n    filename: str\n    page_start: int\n    page_end: int\n    distance: float\n    relevance: float\n    rerank_score: float  # NEW: re-ranking score\n    citation: str\n\n@dataclass\nclass AdvancedRAGResponse:\n    question: str\n    expanded_queries: List[str]  # NEW: query variants\n    answer: str\n    citations: List[str]\n    retrieved_chunks: List[RetrievedChunk]\n    avg_relevance: float\n    avg_rerank_score: float  # NEW\n    answer_length: int\n    \n    def to_dict(self):\n        return {\n            'question': self.question,\n            'expanded_queries': self.expanded_queries,\n            'answer': self.answer,\n            'citations': self.citations,\n            'avg_relevance': self.avg_relevance,\n            'avg_rerank_score': self.avg_rerank_score,\n            'answer_length': self.answer_length,\n            'num_chunks': len(self.retrieved_chunks)\n        }\n\nprint(\"Dataclasses defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"LOADING VECTOR DATABASE\")\nprint(\"=\"*80)\n\nclient = chromadb.PersistentClient(path=str(DB_PATH))\n\nembedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n    model_name=EMBEDDING_MODEL\n)\n\ncollection = client.get_collection(\n    name=COLLECTION_NAME,\n    embedding_function=embedding_function\n)\n\nprint(f\"\\nCollection: {COLLECTION_NAME}\")\nprint(f\"  Total chunks: {collection.count():,}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"LOADING LLM\")\nprint(\"=\"*80)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\nmodel = AutoModelForCausalLM.from_pretrained(\n    str(MODEL_PATH),\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\nprint(\"Model loaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Query Expansion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def expand_query(query: str, num_variants: int = NUM_QUERY_EXPANSIONS) -> List[str]:\n    \"\"\"\n    Generate multiple query variants for better retrieval coverage.\n    \n    Strategy:\n    1. Original query\n    2. Extract key math terms and rephrase\n    3. Add context (e.g., \"формула\", \"приклад\", \"визначення\")\n    \"\"\"\n    queries = [query]  # Always include original\n    \n    # Generate variants using simple LLM prompting\n    expansion_prompt = f\"\"\"Перефразуй це запитання українською мовою {num_variants-1} різними способами, \nзберігаючи математичний зміст. Використовуй різні формулювання та синоніми.\n\nОригінальне запитання: {query}\n\nВаріанти (по одному на рядок):\"\"\"\n    \n    messages = [{\"role\": \"user\", \"content\": expansion_prompt}]\n    formatted = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=150,\n            temperature=0.8,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    result = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n    \n    # Parse variants (split by newlines)\n    variants = [line.strip() for line in result.split('\\n') if line.strip()]\n    queries.extend(variants[:(num_variants-1)])\n    \n    return queries[:num_variants]\n\nprint(\"Query expansion function defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hybrid Retrieval & Re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def retrieve_with_queries(\n    queries: List[str],\n    k: int = RETRIEVAL_K\n) -> List[RetrievedChunk]:\n    \"\"\"\n    Retrieve chunks using multiple queries and merge results.\n    \"\"\"\n    all_chunks = {}\n    \n    for query in queries:\n        results = collection.query(\n            query_texts=[query],\n            n_results=k\n        )\n        \n        for doc, meta, dist in zip(\n            results['documents'][0],\n            results['metadatas'][0],\n            results['distances'][0]\n        ):\n            # Use text as key to deduplicate\n            key = doc[:100]  # First 100 chars as key\n            \n            if key not in all_chunks:\n                chunk = RetrievedChunk(\n                    text=doc,\n                    content_type=meta['content_type'],\n                    confidence=meta['confidence'],\n                    filename=meta['filename'],\n                    page_start=meta['page_start'],\n                    page_end=meta['page_end'],\n                    distance=dist,\n                    relevance=1 - dist,\n                    rerank_score=0.0,  # Will be computed later\n                    citation=f\"[{meta['filename']}, с. {meta['page_start']}-{meta['page_end']}]\"\n                )\n                all_chunks[key] = chunk\n            else:\n                # Update with better relevance if found\n                if (1 - dist) > all_chunks[key].relevance:\n                    all_chunks[key].distance = dist\n                    all_chunks[key].relevance = 1 - dist\n    \n    return list(all_chunks.values())\n\nprint(\"Hybrid retrieval function defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def calculate_diversity_score(chunks: List[RetrievedChunk]) -> List[float]:\n    \"\"\"\n    Calculate diversity scores based on content type and source variety.\n    \"\"\"\n    # Count occurrences\n    type_counts = Counter(c.content_type for c in chunks)\n    file_counts = Counter(c.filename for c in chunks)\n    \n    diversity_scores = []\n    for chunk in chunks:\n        # Penalize over-represented types and files\n        type_penalty = 1.0 / type_counts[chunk.content_type]\n        file_penalty = 1.0 / file_counts[chunk.filename]\n        diversity_score = (type_penalty + file_penalty) / 2\n        diversity_scores.append(diversity_score)\n    \n    # Normalize to [0, 1]\n    max_score = max(diversity_scores) if diversity_scores else 1.0\n    return [s / max_score for s in diversity_scores]\n\ndef calculate_content_type_score(chunk: RetrievedChunk) -> float:\n    \"\"\"\n    Score chunks by content type preference for task generation.\n    \n    Preference order:\n    1. explanation (best for understanding concepts)\n    2. definition (good for terminology)\n    3. problem (examples of tasks)\n    4. example, theorem, etc.\n    \"\"\"\n    type_scores = {\n        'explanation': 1.0,\n        'definition': 0.9,\n        'problem': 0.8,\n        'example': 0.7,\n        'theorem': 0.7,\n        'formula': 0.6\n    }\n    return type_scores.get(chunk.content_type, 0.5)\n\ndef rerank_chunks(\n    chunks: List[RetrievedChunk],\n    final_k: int = FINAL_K\n) -> List[RetrievedChunk]:\n    \"\"\"\n    Re-rank chunks using weighted combination of:\n    - Semantic relevance (from embedding distance)\n    - Content diversity (variety of types/sources)\n    - Content type preference (explanations > examples)\n    \"\"\"\n    diversity_scores = calculate_diversity_score(chunks)\n    \n    for i, chunk in enumerate(chunks):\n        relevance = chunk.relevance\n        diversity = diversity_scores[i]\n        content_type = calculate_content_type_score(chunk)\n        \n        # Weighted combination\n        rerank_score = (\n            RELEVANCE_WEIGHT * relevance +\n            DIVERSITY_WEIGHT * diversity +\n            CONTENT_TYPE_WEIGHT * content_type\n        )\n        chunk.rerank_score = rerank_score\n    \n    # Sort by rerank score and take top-k\n    chunks.sort(key=lambda c: c.rerank_score, reverse=True)\n    return chunks[:final_k]\n\nprint(\"Re-ranking functions defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "SYSTEM_PROMPT = \"\"\"Ти — досвідчений викладач математики для українських учнів 10-11 класів.\n\nТвоє завдання:\n- Згенерувати математичну задачу з розв'язанням на основі ТІЛЬКИ наданого контексту\n- Використовувати ТІЛЬКИ українську мову\n- Використовувати математичну термінологію з підручників\n- Обов'язково посилатися на джерела\n- Надати чітке покрокове розв'язання\n\nФормат відповіді:\n**Задача:** [текст задачі на основі контексту]\n\n**Розв'язання:**\n[покрокове рішення з посиланнями на джерела]\n\n**Відповідь:** [фінальна відповідь]\n\nВАЖЛИВО: Використовуй ТІЛЬКИ інформацію з наданого контексту!\"\"\"\n\ndef format_context(chunks: List[RetrievedChunk]) -> str:\n    \"\"\"Format chunks with re-rank scores.\"\"\"\n    context_parts = []\n    for i, chunk in enumerate(chunks, 1):\n        header = f\"[Джерело {i}] {chunk.citation} | Тип: {chunk.content_type} | Оцінка: {chunk.rerank_score:.3f}\"\n        context_parts.append(f\"{header}\\n{chunk.text}\")\n    return \"\\n\\n\".join(context_parts)\n\ndef generate_answer(prompt: str) -> str:\n    \"\"\"Generate using LLM.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    formatted = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            temperature=TEMPERATURE,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    return tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n\ndef advanced_rag_generate(\n    question: str,\n    verbose: bool = False\n) -> AdvancedRAGResponse:\n    \"\"\"\n    Advanced RAG pipeline with query expansion and re-ranking.\n    \"\"\"\n    if verbose:\n        print(f\"\\nQuestion: {question}\")\n        print(\"  Step 1: Query expansion...\")\n    \n    # 1. Query expansion\n    expanded_queries = expand_query(question)\n    if verbose:\n        for i, q in enumerate(expanded_queries, 1):\n            print(f\"    {i}. {q}\")\n        print(f\"  Step 2: Hybrid retrieval (k={RETRIEVAL_K})...\")\n    \n    # 2. Retrieve with multiple queries\n    chunks = retrieve_with_queries(expanded_queries)\n    if verbose:\n        print(f\"    Retrieved {len(chunks)} unique chunks\")\n        print(f\"  Step 3: Re-ranking to top-{FINAL_K}...\")\n    \n    # 3. Re-rank\n    reranked_chunks = rerank_chunks(chunks, final_k=FINAL_K)\n    if verbose:\n        avg_rel = np.mean([c.relevance for c in reranked_chunks])\n        avg_rerank = np.mean([c.rerank_score for c in reranked_chunks])\n        print(f\"    Avg relevance: {avg_rel:.3f} | Avg rerank score: {avg_rerank:.3f}\")\n        print(\"  Step 4: Generating answer...\")\n    \n    # 4. Generate\n    context = format_context(reranked_chunks)\n    prompt = f\"{SYSTEM_PROMPT}\\n\\nКОНТЕКСТ З ПІДРУЧНИКІВ:\\n{context}\\n\\nЗАПИТАННЯ:\\n{question}\\n\\nТВОЯ ВІДПОВІДЬ:\"\n    answer = generate_answer(prompt)\n    \n    if verbose:\n        print(f\"    Generated {len(answer)} chars\")\n    \n    return AdvancedRAGResponse(\n        question=question,\n        expanded_queries=expanded_queries,\n        answer=answer,\n        citations=[c.citation for c in reranked_chunks],\n        retrieved_chunks=reranked_chunks,\n        avg_relevance=float(np.mean([c.relevance for c in reranked_chunks])),\n        avg_rerank_score=float(np.mean([c.rerank_score for c in reranked_chunks])),\n        answer_length=len(answer)\n    )\n\nprint(\"Advanced RAG pipeline defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from common import STANDARD_TEST_QUESTIONS, EVALUATION_DATASET\n\nTEST_QUESTIONS = STANDARD_TEST_QUESTIONS\nprint(f\"Test set: {len(TEST_QUESTIONS)} questions\")\n\n# Create mapping of questions to expected answers\nquestion_to_expected = {q['input']: q['expected_answer'] for q in EVALUATION_DATASET}\nprint(f\"Expected answers loaded for {len(question_to_expected)} questions\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Advanced RAG Experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"RUNNING ADVANCED RAG EXPERIMENT\")\nprint(\"=\"*80)\n\nresponses = []\n\nfor i, question in enumerate(TEST_QUESTIONS, 1):\n    print(f\"\\n[{i}/{len(TEST_QUESTIONS)}] {question}\")\n    print(\"-\"*80)\n    \n    response = advanced_rag_generate(question, verbose=True)\n    responses.append(response)\n    \n    print(f\"\\nAnswer:\\n{response.answer}\")\n    print(f\"\\nTop-3 Citations:\")\n    for j, citation in enumerate(response.citations[:3], 1):\n        print(f\"  {j}. {citation}\")\n\nprint(f\"\\n{'='*80}\")\nprint(f\"Completed {len(responses)} advanced RAG responses\")\nprint(\"=\"*80)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import common\n\nprint(\"Evaluation functions loaded from common.py\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate\nprint(\"=\"*80)\nprint(\"EVALUATION\")\nprint(\"=\"*80)\n\nevaluations = []\n\nfor i, response in enumerate(responses, 1):\n    expected_answer = question_to_expected.get(response.question, None)\n    metrics = common.evaluate_advanced_rag(\n        response.answer, \n        response.answer_length, \n        response.avg_relevance, \n        response.avg_rerank_score,\n        expected_answer\n    )\n    evaluations.append({\n        'question': response.question,\n        'metrics': metrics,\n        'answer_length': response.answer_length,\n        'avg_relevance': response.avg_relevance,\n        'avg_rerank': response.avg_rerank_score\n    })\n    \n    print(f\"\\n{i}. {response.question[:50]}...\")\n    print(f\"   Overall: {metrics['overall_score']:.3f} | \"\n          f\"Rerank: {metrics['rerank_quality']:.3f} | \"\n          f\"Ukrainian: {metrics['ukrainian_ratio']:.3f}\")\n\n# Summary\nprint(f\"\\n{'='*80}\")\nprint(\"SUMMARY\")\nprint(\"=\"*80)\n\navg_metrics = {\n    'overall_score': np.mean([e['metrics']['overall_score'] for e in evaluations]),\n    'retrieval_quality': np.mean([e['metrics']['retrieval_quality'] for e in evaluations]),\n    'rerank_quality': np.mean([e['metrics']['rerank_quality'] for e in evaluations]),\n    'ukrainian_ratio': np.mean([e['metrics']['ukrainian_ratio'] for e in evaluations]),\n    'completeness': np.mean([e['metrics']['completeness'] for e in evaluations]),\n    'correctness': np.mean([e['metrics']['correctness'] for e in evaluations]),\n    'structure_rate': sum(e['metrics']['has_structure'] for e in evaluations) / len(evaluations),\n    'citation_rate': sum(e['metrics']['has_citations'] for e in evaluations) / len(evaluations)\n}\n\nfor key, value in avg_metrics.items():\n    print(f\"  {key:20s}: {value:.3f}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = {\n    'experiment': 'advanced_rag',\n    'description': 'Query expansion + hybrid retrieval + re-ranking',\n    'model': 'gemma-2-9b-it',\n    'config': {\n        'query_expansions': NUM_QUERY_EXPANSIONS,\n        'retrieval_k': RETRIEVAL_K,\n        'final_k': FINAL_K,\n        'relevance_weight': RELEVANCE_WEIGHT,\n        'diversity_weight': DIVERSITY_WEIGHT,\n        'content_type_weight': CONTENT_TYPE_WEIGHT\n    },\n    'avg_metrics': avg_metrics,\n    'responses': [r.to_dict() for r in responses],\n    'evaluations': evaluations\n}\n\nwith open(OUTPUT_DIR / 'results.json', 'w', encoding='utf-8') as f:\n    json.dump(results, f, ensure_ascii=False, indent=2)\n\nprint(f\"Results saved to {OUTPUT_DIR}\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPERIMENT 3 COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\\nOverall Score: {avg_metrics['overall_score']:.3f}\")\nprint(f\"Retrieval Quality: {avg_metrics['retrieval_quality']:.3f}\")\nprint(f\"Re-rank Quality: {avg_metrics['rerank_quality']:.3f}\")\nprint(f\"Ukrainian Ratio: {avg_metrics['ukrainian_ratio']:.3f}\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
