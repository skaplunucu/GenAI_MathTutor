{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 4: RAG + Tool Use"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Setup\nimport sys\nimport json\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, asdict\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Wolfram Alpha\nimport requests\n\n# RAG\nimport chromadb\nfrom chromadb.utils import embedding_functions\n\n# LLM\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nsys.path.append('..')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\n\nprint(\"Imports loaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "DB_PATH = Path(\"../data/vector_db\")\n",
    "MODEL_PATH = Path(\"/home/sskaplun/study/genAI/kaggle/models/gemma-2-9b-it\")\n",
    "OUTPUT_DIR = Path(\"../evaluation/experiment_04\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "COLLECTION_NAME = \"ukrainian_math\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# Wolfram Alpha API\n",
    "WOLFRAM_APP_ID = 'YL2L8P8W5J'\n",
    "\n",
    "# RAG parameters\n",
    "TOP_K = 5\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 600\n",
    "\n",
    "print(f\"Wolfram API: {'Configured' if WOLFRAM_APP_ID != 'DEMO' else 'DEMO mode (limited)'}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "@dataclass\nclass ToolCall:\n    tool_name: str\n    query: str\n    result: str\n    success: bool\n\n@dataclass\nclass RAGToolResponse:\n    question: str\n    answer: str\n    citations: List[str]\n    tool_calls: List[ToolCall]  # NEW: track tool usage\n    avg_relevance: float\n    answer_length: int\n    verified: bool  # NEW: was answer computationally verified?\n    \n    def to_dict(self):\n        return {\n            'question': self.question,\n            'answer': self.answer,\n            'citations': self.citations,\n            'tool_calls': [asdict(t) for t in self.tool_calls],\n            'avg_relevance': self.avg_relevance,\n            'answer_length': self.answer_length,\n            'verified': self.verified\n        }\n\nprint(\"Dataclasses defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"LOADING VECTOR DATABASE\")\nprint(\"=\"*80)\n\nclient = chromadb.PersistentClient(path=str(DB_PATH))\nembedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n    model_name=EMBEDDING_MODEL\n)\ncollection = client.get_collection(\n    name=COLLECTION_NAME,\n    embedding_function=embedding_function\n)\n\nprint(f\"Collection loaded: {collection.count():,} chunks\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"LOADING LLM\")\nprint(\"=\"*80)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\nmodel = AutoModelForCausalLM.from_pretrained(\n    str(MODEL_PATH),\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\nprint(\"Model loaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wolfram Alpha Tool"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def query_wolfram_alpha(query: str, timeout: int = 10) -> ToolCall:\n    \"\"\"\n    Query Wolfram Alpha API for mathematical computations.\n    \n    Args:\n        query: Natural language or symbolic math query\n        timeout: Request timeout in seconds\n    \n    Returns:\n        ToolCall with result or error message\n    \"\"\"\n    if WOLFRAM_APP_ID == 'DEMO':\n        # DEMO mode: simulate responses\n        return ToolCall(\n            tool_name=\"wolfram_alpha\",\n            query=query,\n            result=f\"DEMO: Would compute '{query}' (set WOLFRAM_APP_ID for real queries)\",\n            success=False\n        )\n    \n    try:\n        # Wolfram Alpha Simple API (HTTPS required)\n        url = \"https://api.wolframalpha.com/v1/result\"\n        params = {\n            'appid': WOLFRAM_APP_ID,\n            'i': query\n        }\n        \n        response = requests.get(url, params=params, timeout=timeout)\n        \n        if response.status_code == 200:\n            result = response.text\n            return ToolCall(\n                tool_name=\"wolfram_alpha\",\n                query=query,\n                result=result,\n                success=True\n            )\n        else:\n            return ToolCall(\n                tool_name=\"wolfram_alpha\",\n                query=query,\n                result=f\"Error {response.status_code}: {response.text}\",\n                success=False\n            )\n    \n    except Exception as e:\n        return ToolCall(\n            tool_name=\"wolfram_alpha\",\n            query=query,\n            result=f\"Exception: {str(e)}\",\n            success=False\n        )\n\nprint(\"Wolfram Alpha tool defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Test Wolfram Alpha\n",
    "test_queries = [\n",
    "    \"integrate x^2\",\n",
    "    \"solve x^2 + 5x + 6 = 0\",\n",
    "    \"volume of sphere with radius 5\"\n",
    "]\n",
    "\n",
    "print(\"Testing Wolfram Alpha API:\")\n",
    "print(\"=\"*80)\n",
    "for query in test_queries:\n",
    "    result = query_wolfram_alpha(query)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Success: {result.success}\")\n",
    "    print(f\"Result: {result.result}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG + Tool Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def retrieve_chunks(query: str, k: int = TOP_K) -> tuple:\n    \"\"\"Retrieve from vector DB.\"\"\"\n    results = collection.query(query_texts=[query], n_results=k)\n    \n    chunks = []\n    citations = []\n    \n    for doc, meta, dist in zip(\n        results['documents'][0],\n        results['metadatas'][0],\n        results['distances'][0]\n    ):\n        citation = f\"[{meta['filename']}, с. {meta['page_start']}-{meta['page_end']}]\"\n        header = f\"[Джерело {len(chunks)+1}] {citation} | Тип: {meta['content_type']}\"\n        chunks.append(f\"{header}\\n{doc}\")\n        citations.append(citation)\n    \n    context = \"\\n\\n\".join(chunks)\n    avg_relevance = float(np.mean([1 - d for d in results['distances'][0]]))\n    \n    return context, citations, avg_relevance\n\nprint(\"Retrieval function defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "SYSTEM_PROMPT = \"\"\"Ти — досвідчений викладач математики для українських учнів 10-11 класів.\n\nТи маєш доступ до інструментів:\n1. **Підручники** (контекст нижче) - для теорії та формул\n2. **Wolfram Alpha** - для обчислень та перевірки\n\nТвоє завдання:\n- Згенерувати математичну задачу з ПЕРЕВІРЕНИМ розв'язанням\n- Використовувати формули з підручників\n- ОБОВ'ЯЗКОВО використати Wolfram Alpha для перевірки обчислень\n- Надати крок-за-кроком розв'язання українською мовою\n\nЯк використовувати Wolfram Alpha:\nНапиши: [WOLFRAM: твій запит]\nПриклад: [WOLFRAM: volume of sphere with radius 5]\n\nФормат відповіді:\n**Задача:** [текст задачі]\n\n**Розв'язання:**\n1. [крок 1]\n2. [крок 2]\n[WOLFRAM: обчислення для перевірки]\n3. [крок 3]\n\n**Відповідь:** [фінальна відповідь]\n\nВАЖЛИВО: \n- Використай контекст з підручників\n- ОБОВ'ЯЗКОВО використай Wolfram Alpha хоча б раз\n- Відповідай ТІЛЬКИ українською\"\"\"\n\nprint(\"System prompt defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def extract_wolfram_queries(text: str) -> List[str]:\n    \"\"\"Extract Wolfram Alpha queries from text.\"\"\"\n    pattern = r'\\[WOLFRAM:\\s*([^\\]]+)\\]'\n    matches = re.findall(pattern, text, re.IGNORECASE)\n    return [m.strip() for m in matches]\n\ndef generate_with_llm(prompt: str) -> str:\n    \"\"\"Generate text using LLM.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    formatted = tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=MAX_NEW_TOKENS,\n            temperature=TEMPERATURE,\n            top_p=0.9,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    return tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n\nprint(\"Helper functions defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def rag_tool_generate(\n    question: str,\n    verbose: bool = False\n) -> RAGToolResponse:\n    \"\"\"\n    RAG + Tool pipeline:\n    1. Retrieve context from textbooks\n    2. Generate initial answer with tool calls\n    3. Execute Wolfram queries\n    4. Refine answer with tool results\n    \"\"\"\n    if verbose:\n        print(f\"\\nQuestion: {question}\")\n        print(\"  Step 1: Retrieving context...\")\n    \n    # 1. Retrieve\n    context, citations, avg_relevance = retrieve_chunks(question)\n    \n    if verbose:\n        print(f\"    Retrieved {len(citations)} chunks\")\n        print(\"  Step 2: Generating with tool calls...\")\n    \n    # 2. Generate with tool instructions\n    prompt = f\"{SYSTEM_PROMPT}\\n\\nКОНТЕКСТ З ПІДРУЧНИКІВ:\\n{context}\\n\\nЗАПИТАННЯ:\\n{question}\\n\\nТВОЯ ВІДПОВІДЬ:\"\n    answer = generate_with_llm(prompt)\n    \n    if verbose:\n        print(f\"    Generated {len(answer)} chars\")\n        print(\"  Step 3: Extracting tool calls...\")\n    \n    # 3. Extract and execute Wolfram queries\n    wolfram_queries = extract_wolfram_queries(answer)\n    tool_calls = []\n    \n    if verbose and wolfram_queries:\n        print(f\"    Found {len(wolfram_queries)} Wolfram queries\")\n    \n    for wq in wolfram_queries:\n        if verbose:\n            print(f\"      Querying: {wq}\")\n        tool_call = query_wolfram_alpha(wq)\n        tool_calls.append(tool_call)\n        \n        # Replace placeholder with result\n        placeholder = f\"[WOLFRAM: {wq}]\"\n        replacement = f\"[WOLFRAM RESULT: {tool_call.result}]\"\n        answer = answer.replace(placeholder, replacement)\n    \n    verified = any(tc.success for tc in tool_calls)\n    \n    if verbose:\n        print(f\"    Verified: {verified}\")\n    \n    return RAGToolResponse(\n        question=question,\n        answer=answer,\n        citations=citations,\n        tool_calls=tool_calls,\n        avg_relevance=avg_relevance,\n        answer_length=len(answer),\n        verified=verified\n    )\n\nprint(\"RAG+Tool pipeline defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from common import TOOL_TEST_QUESTIONS, EVALUATION_DATASET\n\nTEST_QUESTIONS = TOOL_TEST_QUESTIONS\nprint(f\"Test set: {len(TEST_QUESTIONS)} questions\")\n\n# Create mapping of questions to expected answers\nquestion_to_expected = {q['input']: q['expected_answer'] for q in EVALUATION_DATASET}\nprint(f\"Expected answers loaded for {len(question_to_expected)} questions\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"RUNNING RAG + TOOL USE EXPERIMENT\")\nprint(\"=\"*80)\n\nresponses = []\n\nfor i, question in enumerate(TEST_QUESTIONS, 1):\n    print(f\"\\n[{i}/{len(TEST_QUESTIONS)}] {question}\")\n    print(\"-\"*80)\n    \n    response = rag_tool_generate(question, verbose=True)\n    responses.append(response)\n    \n    print(f\"\\nAnswer:\\n{response.answer}\")\n    print(f\"\\nTool Calls: {len(response.tool_calls)}\")\n    for tc in response.tool_calls:\n        print(f\"  - {tc.tool_name}: {tc.query[:50]}... → {tc.success}\")\n    print(f\"\\nVerified: {response.verified}\")\n\nprint(f\"\\n{'='*80}\")\nprint(f\"Completed {len(responses)} RAG+Tool responses\")\nprint(\"=\"*80)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import common\n\nprint(\"Evaluation functions loaded from common.py\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate\nprint(\"=\"*80)\nprint(\"EVALUATION\")\nprint(\"=\"*80)\n\nevaluations = []\n\nfor i, response in enumerate(responses, 1):\n    expected_answer = question_to_expected.get(response.question, None)\n    metrics = common.evaluate_rag_tools(\n        response.answer, \n        response.answer_length, \n        response.avg_relevance, \n        len(response.tool_calls) > 0, \n        response.verified,\n        expected_answer\n    )\n    evaluations.append({\n        'question': response.question,\n        'metrics': metrics,\n        'answer_length': response.answer_length,\n        'num_tool_calls': len(response.tool_calls)\n    })\n    \n    print(f\"\\n{i}. {response.question[:50]}...\")\n    print(f\"   Overall: {metrics['overall_score']:.3f} | \"\n          f\"Tools: {metrics['tool_usage']} | \"\n          f\"Verified: {metrics['verified']}\")\n\n# Summary\nprint(f\"\\n{'='*80}\")\nprint(\"SUMMARY\")\nprint(\"=\"*80)\n\navg_metrics = {\n    'overall_score': np.mean([e['metrics']['overall_score'] for e in evaluations]),\n    'retrieval_quality': np.mean([e['metrics']['retrieval_quality'] for e in evaluations]),\n    'ukrainian_ratio': np.mean([e['metrics']['ukrainian_ratio'] for e in evaluations]),\n    'completeness': np.mean([e['metrics']['completeness'] for e in evaluations]),\n    'correctness': np.mean([e['metrics']['correctness'] for e in evaluations]),\n    'structure_rate': sum(e['metrics']['has_structure'] for e in evaluations) / len(evaluations),\n    'citation_rate': sum(e['metrics']['has_citations'] for e in evaluations) / len(evaluations),\n    'tool_usage_rate': sum(e['metrics']['tool_usage'] for e in evaluations) / len(evaluations),\n    'verification_rate': sum(e['metrics']['verified'] for e in evaluations) / len(evaluations),\n    'avg_tool_calls': np.mean([e['num_tool_calls'] for e in evaluations])\n}\n\nfor key, value in avg_metrics.items():\n    print(f\"  {key:20s}: {value:.3f}\")\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = {\n    'experiment': 'rag_with_tools',\n    'description': 'RAG + Wolfram Alpha for verified computations',\n    'model': 'gemma-2-9b-it',\n    'tools': ['wolfram_alpha'],\n    'wolfram_mode': 'DEMO' if WOLFRAM_APP_ID == 'DEMO' else 'API',\n    'avg_metrics': avg_metrics,\n    'responses': [r.to_dict() for r in responses],\n    'evaluations': evaluations\n}\n\nwith open(OUTPUT_DIR / 'results.json', 'w', encoding='utf-8') as f:\n    json.dump(results, f, ensure_ascii=False, indent=2)\n\nprint(f\"Results saved to {OUTPUT_DIR}\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPERIMENT 4 COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\\nOverall Score: {avg_metrics['overall_score']:.3f}\")\nprint(f\"Tool Usage Rate: {avg_metrics['tool_usage_rate']:.3f}\")\nprint(f\"Verification Rate: {avg_metrics['verification_rate']:.3f}\")\nprint(f\"Avg Tool Calls: {avg_metrics['avg_tool_calls']:.1f}\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
