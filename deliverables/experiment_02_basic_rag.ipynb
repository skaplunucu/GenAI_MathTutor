{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Experiment 2: Basic RAG"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Setup\nimport sys\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, asdict\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport chromadb\nfrom chromadb.utils import embedding_functions\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nsys.path.append('..')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"Imports loaded\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "DB_PATH = Path(\"../data/vector_db\")\n",
    "MODEL_PATH = Path(\"/home/sskaplun/study/genAI/kaggle/models/gemma-2-9b-it\")\n",
    "OUTPUT_DIR = Path(\"../evaluation/experiment_02\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "COLLECTION_NAME = \"ukrainian_math\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# RAG parameters\n",
    "TOP_K = 5\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 512\n",
    "\n",
    "print(f\"Database: {DB_PATH}\")\n",
    "print(f\"Model: {MODEL_PATH}\")\n",
    "print(f\"Top-K: {TOP_K}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "@dataclass\nclass RetrievedChunk:\n    text: str\n    content_type: str\n    confidence: float\n    filename: str\n    page_start: int\n    page_end: int\n    distance: float\n    relevance: float\n    citation: str\n\n@dataclass\nclass RAGResponse:\n    question: str\n    answer: str\n    citations: List[str]\n    retrieved_chunks: List[RetrievedChunk]\n    avg_relevance: float\n    answer_length: int\n    \n    def to_dict(self):\n        return {\n            'question': self.question,\n            'answer': self.answer,\n            'citations': self.citations,\n            'avg_relevance': self.avg_relevance,\n            'answer_length': self.answer_length,\n            'num_chunks': len(self.retrieved_chunks)\n        }\n\nprint(\"Dataclasses defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"LOADING VECTOR DATABASE\")\nprint(\"=\"*80)\n\nclient = chromadb.PersistentClient(path=str(DB_PATH))\n\nembedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n    model_name=EMBEDDING_MODEL\n)\n\ncollection = client.get_collection(\n    name=COLLECTION_NAME,\n    embedding_function=embedding_function\n)\n\ncount = collection.count()\nmetadata = collection.metadata\n\nprint(f\"\\nVector database loaded\")\nprint(f\"  Collection: {COLLECTION_NAME}\")\nprint(f\"  Total chunks: {count:,}\")\nprint(f\"  Metadata: {metadata}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"LOADING LLM\")\nprint(\"=\"*80)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\ntokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\nprint(\"Tokenizer loaded\")\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    str(MODEL_PATH),\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\nprint(\"Model loaded\")\n\nif torch.cuda.is_available():\n    memory = torch.cuda.memory_allocated() / 1024**3\n    print(f\"GPU Memory: {memory:.2f} GB\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "SYSTEM_PROMPT = \"\"\"Ти — досвідчений викладач математики для українських учнів 10-11 класів.\n\nТвоє завдання:\n- Згенерувати математичну задачу з розв'язанням на основі ТІЛЬКИ наданого контексту\n- Використовувати ТІЛЬКИ українську мову\n- Використовувати математичну термінологію з підручників\n- Обов'язково посилатися на джерела (наприклад: \"За формулою [Джерело 1]...\")\n- Надати чітке покрокове розв'язання\n\nФормат відповіді:\n**Задача:** [текст задачі на основі контексту]\n\n**Розв'язання:**\n[покрокове рішення з посиланнями на джерела]\n\n**Відповідь:** [фінальна відповідь]\n\nВАЖЛИВО: Використовуй ТІЛЬКИ інформацію з наданого контексту!\"\"\"\n\nprint(\"System prompt defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def retrieve_chunks(query: str, k: int = TOP_K) -> List[RetrievedChunk]:\n    \"\"\"Retrieve relevant chunks from vector database.\"\"\"\n    results = collection.query(\n        query_texts=[query],\n        n_results=k\n    )\n    \n    chunks = []\n    for doc, meta, dist in zip(\n        results['documents'][0],\n        results['metadatas'][0],\n        results['distances'][0]\n    ):\n        chunk = RetrievedChunk(\n            text=doc,\n            content_type=meta['content_type'],\n            confidence=meta['confidence'],\n            filename=meta['filename'],\n            page_start=meta['page_start'],\n            page_end=meta['page_end'],\n            distance=dist,\n            relevance=1 - dist,\n            citation=f\"[{meta['filename']}, с. {meta['page_start']}-{meta['page_end']}]\"\n        )\n        chunks.append(chunk)\n    \n    return chunks\n\nprint(\"Retrieval function defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def format_context(chunks: List[RetrievedChunk]) -> str:\n    \"\"\"Format chunks into context for LLM.\"\"\"\n    context_parts = []\n    \n    for i, chunk in enumerate(chunks, 1):\n        header = f\"[Джерело {i}] {chunk.citation} | Тип: {chunk.content_type}\"\n        context_parts.append(f\"{header}\\n{chunk.text}\")\n    \n    return \"\\n\\n\".join(context_parts)\n\nprint(\"Context formatter defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def generate_with_context(\n    prompt: str,\n    temperature: float = TEMPERATURE,\n    max_new_tokens: int = MAX_NEW_TOKENS\n) -> str:\n    \"\"\"Generate answer using LLM.\"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]\n    formatted = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_p=0.9,\n            do_sample=temperature > 0,\n            pad_token_id=tokenizer.eos_token_id\n        )\n    \n    answer = tokenizer.decode(\n        outputs[0][inputs['input_ids'].shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n    \n    return answer\n\nprint(\"Generation function defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def rag_generate(\n    question: str,\n    k: int = TOP_K,\n    temperature: float = TEMPERATURE,\n    verbose: bool = False\n) -> RAGResponse:\n    \"\"\"Complete RAG pipeline: retrieve, format, generate.\"\"\"\n    if verbose:\n        print(f\"Retrieving {k} chunks for: {question}\")\n    \n    chunks = retrieve_chunks(question, k=k)\n    \n    if verbose:\n        avg_rel = np.mean([c.relevance for c in chunks])\n        print(f\"  Avg relevance: {avg_rel:.3f}\")\n    \n    context = format_context(chunks)\n    \n    prompt = f\"{SYSTEM_PROMPT}\\n\\nКОНТЕКСТ З ПІДРУЧНИКІВ:\\n{context}\\n\\nЗАПИТАННЯ:\\n{question}\\n\\nТВОЯ ВІДПОВІДЬ:\"\n    \n    if verbose:\n        print(f\"  Prompt length: {len(prompt)} chars\")\n        print(\"  Generating...\")\n    \n    answer = generate_with_context(prompt, temperature=temperature)\n    \n    if verbose:\n        print(f\"  Answer: {len(answer)} chars\")\n    \n    return RAGResponse(\n        question=question,\n        answer=answer,\n        citations=[c.citation for c in chunks],\n        retrieved_chunks=chunks,\n        avg_relevance=float(np.mean([c.relevance for c in chunks])),\n        answer_length=len(answer)\n    )\n\nprint(\"RAG pipeline defined\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from common import STANDARD_TEST_QUESTIONS, EVALUATION_DATASET\n\nTEST_QUESTIONS = STANDARD_TEST_QUESTIONS\nprint(f\"Test set: {len(TEST_QUESTIONS)} questions\")\n\n# Create mapping of questions to expected answers\nquestion_to_expected = {q['input']: q['expected_answer'] for q in EVALUATION_DATASET}\nprint(f\"Expected answers loaded for {len(question_to_expected)} questions\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run RAG Experiment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "print(\"=\"*80)\nprint(\"RUNNING BASIC RAG EXPERIMENT\")\nprint(\"=\"*80)\n\nresponses = []\n\nfor i, question in enumerate(TEST_QUESTIONS, 1):\n    print(f\"\\n[{i}/{len(TEST_QUESTIONS)}] {question}\")\n    print(\"-\"*80)\n    \n    response = rag_generate(question, verbose=True)\n    responses.append(response)\n    \n    print(f\"\\nAnswer:\\n{response.answer}\")\n    print(f\"\\nCitations: {len(response.citations)}\")\n    for j, citation in enumerate(response.citations[:3], 1):\n        print(f\"  {j}. {citation}\")\n\nprint(f\"\\n{'='*80}\")\nprint(f\"Generated {len(responses)} RAG responses\")\nprint(\"=\"*80)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import common\n\nprint(\"Evaluation functions loaded from common.py\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Evaluate\nprint(\"=\"*80)\nprint(\"EVALUATION\")\nprint(\"=\"*80)\n\nevaluations = []\n\nfor i, response in enumerate(responses, 1):\n    # Get expected answer for this question\n    expected = question_to_expected.get(response.question, None)\n    \n    # Evaluate with expected answer for correctness\n    metrics = common.evaluate_basic_rag(\n        response.answer,\n        response.answer_length,\n        response.avg_relevance,\n        expected_answer=expected\n    )\n    \n    evaluations.append({\n        'question': response.question,\n        'metrics': metrics,\n        'answer_length': response.answer_length,\n        'avg_relevance': response.avg_relevance,\n        'expected_answer': expected\n    })\n    \n    print(f\"\\n{i}. {response.question[:50]}...\")\n    print(f\"   Overall: {metrics['overall_score']:.3f} | \"\n          f\"Retrieval: {metrics['retrieval_quality']:.3f} | \"\n          f\"Correctness: {metrics['correctness']:.3f} | \"\n          f\"Ukrainian: {metrics['ukrainian_ratio']:.3f}\")\n\n# Summary\nprint(f\"\\n{'='*80}\")\nprint(\"SUMMARY\")\nprint(\"=\"*80)\n\navg_metrics = {\n    'overall_score': np.mean([e['metrics']['overall_score'] for e in evaluations]),\n    'retrieval_quality': np.mean([e['metrics']['retrieval_quality'] for e in evaluations]),\n    'ukrainian_ratio': np.mean([e['metrics']['ukrainian_ratio'] for e in evaluations]),\n    'completeness': np.mean([e['metrics']['completeness'] for e in evaluations]),\n    'structure_rate': sum(e['metrics']['has_structure'] for e in evaluations) / len(evaluations),\n    'citation_rate': sum(e['metrics']['has_citations'] for e in evaluations) / len(evaluations),\n    'correctness': np.mean([e['metrics']['correctness'] for e in evaluations])\n}\n\nfor key, value in avg_metrics.items():\n    print(f\"  {key:20s}: {value:.3f}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        'question_num': i+1,\n",
    "        'question': e['question'][:40] + '...',\n",
    "        'overall': e['metrics']['overall_score'],\n",
    "        'retrieval': e['metrics']['retrieval_quality'],\n",
    "        'ukrainian': e['metrics']['ukrainian_ratio'],\n",
    "        'citations': int(e['metrics']['has_citations']),\n",
    "        'length': e['answer_length']\n",
    "    }\n",
    "    for i, e in enumerate(evaluations)\n",
    "])\n",
    "\n",
    "print(df.to_string())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nax = axes[0]\nbars = ax.bar(df['question_num'], df['overall'], color='green', alpha=0.7, edgecolor='black')\nax.axhline(y=avg_metrics['overall_score'], color='red', linestyle='--',\n           label=f\"Avg: {avg_metrics['overall_score']:.3f}\")\nax.set_xlabel('Question')\nax.set_ylabel('Overall Score')\nax.set_title('Basic RAG: Overall Scores', fontweight='bold')\nax.set_ylim(0, 1.0)\nax.legend()\nax.grid(axis='y', alpha=0.3)\n\nax = axes[1]\nscatter = ax.scatter(df['retrieval'], df['overall'], s=100, alpha=0.7, \n                     c=df['ukrainian'], cmap='RdYlGn', edgecolors='black')\nax.set_xlabel('Retrieval Quality')\nax.set_ylabel('Overall Score')\nax.set_title('Retrieval vs Overall Quality', fontweight='bold')\nax.grid(alpha=0.3)\nplt.colorbar(scatter, ax=ax, label='Ukrainian Ratio')\n\nax = axes[2]\nmetrics_data = [\n    avg_metrics['retrieval_quality'],\n    avg_metrics['ukrainian_ratio'],\n    avg_metrics['citation_rate'],\n    avg_metrics['structure_rate']\n]\nlabels = ['Retrieval', 'Ukrainian', 'Citations', 'Structure']\ncolors = ['#4CAF50', '#2196F3', '#FF9800', '#9C27B0']\nbars = ax.bar(labels, metrics_data, color=colors, alpha=0.7, edgecolor='black')\nax.set_ylabel('Score')\nax.set_title('Average Metrics', fontweight='bold')\nax.set_ylim(0, 1.0)\nax.grid(axis='y', alpha=0.3)\n\nfor bar, val in zip(bars, metrics_data):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig(OUTPUT_DIR / 'rag_metrics.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Visualization saved\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "results = {\n    'experiment': 'basic_rag',\n    'description': 'Vanilla RAG with semantic search and context injection',\n    'model': 'gemma-2-9b-it',\n    'embedding_model': EMBEDDING_MODEL,\n    'top_k': TOP_K,\n    'temperature': TEMPERATURE,\n    'avg_metrics': avg_metrics,\n    'responses': [r.to_dict() for r in responses],\n    'evaluations': evaluations\n}\n\nwith open(OUTPUT_DIR / 'results.json', 'w', encoding='utf-8') as f:\n    json.dump(results, f, ensure_ascii=False, indent=2)\n\ndf.to_csv(OUTPUT_DIR / 'evaluation.csv', index=False)\n\nprint(f\"Results saved to {OUTPUT_DIR}\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"EXPERIMENT 2 COMPLETE\")\nprint(\"=\"*80)\nprint(f\"\\nOverall Score: {avg_metrics['overall_score']:.3f}\")\nprint(f\"Retrieval Quality: {avg_metrics['retrieval_quality']:.3f}\")\nprint(f\"Correctness: {avg_metrics['correctness']:.3f}\")\nprint(f\"Ukrainian Ratio: {avg_metrics['ukrainian_ratio']:.3f}\")\nprint(f\"Citation Rate: {avg_metrics['citation_rate']:.3f}\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
