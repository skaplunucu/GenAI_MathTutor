{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35464b0b",
   "metadata": {
    "papermill": {
     "duration": 0.002093,
     "end_time": "2025-12-23T15:39:26.261100",
     "exception": false,
     "start_time": "2025-12-23T15:39:26.259007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Experiment 5: Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150c2f19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:26.276376Z",
     "iopub.status.busy": "2025-12-23T15:39:26.276284Z",
     "iopub.status.idle": "2025-12-23T15:39:30.443895Z",
     "shell.execute_reply": "2025-12-23T15:39:30.443539Z"
    },
    "papermill": {
     "duration": 4.169835,
     "end_time": "2025-12-23T15:39:30.444345",
     "exception": false,
     "start_time": "2025-12-23T15:39:26.274510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict, field\n",
    "from enum import Enum\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# RAG\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# LLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "sys.path.append('..')\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"Imports loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e70252f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:30.460485Z",
     "iopub.status.busy": "2025-12-23T15:39:30.460234Z",
     "iopub.status.idle": "2025-12-23T15:39:30.498081Z",
     "shell.execute_reply": "2025-12-23T15:39:30.497757Z"
    },
    "papermill": {
     "duration": 0.052138,
     "end_time": "2025-12-23T15:39:30.498352",
     "exception": false,
     "start_time": "2025-12-23T15:39:30.446214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Iterations: 2\n",
      "Quality Threshold: 0.7\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DB_PATH = Path(\"../data/vector_db\")\n",
    "MODEL_PATH = Path(\"/home/sskaplun/study/genAI/kaggle/models/gemma-2-9b-it\")\n",
    "OUTPUT_DIR = Path(\"../evaluation/experiment_05\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "COLLECTION_NAME = \"ukrainian_math\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# Multi-agent parameters\n",
    "TOP_K = 5\n",
    "TEMPERATURE = 0.7\n",
    "MAX_NEW_TOKENS = 512\n",
    "MAX_ITERATIONS = 2  # Max refinement iterations\n",
    "QUALITY_THRESHOLD = 0.7  # Validation threshold\n",
    "\n",
    "print(f\"Max Iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"Quality Threshold: {QUALITY_THRESHOLD}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea1efa8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:30.501970Z",
     "iopub.status.busy": "2025-12-23T15:39:30.501876Z",
     "iopub.status.idle": "2025-12-23T15:39:30.505314Z",
     "shell.execute_reply": "2025-12-23T15:39:30.504983Z"
    },
    "papermill": {
     "duration": 0.005735,
     "end_time": "2025-12-23T15:39:30.505558",
     "exception": false,
     "start_time": "2025-12-23T15:39:30.499823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataclasses defined\n"
     ]
    }
   ],
   "source": [
    "class AgentRole(Enum):\n",
    "    TOPIC = \"topic_agent\"\n",
    "    TASK_GENERATOR = \"task_generator\"\n",
    "    SOLUTION = \"solution_agent\"\n",
    "    QUALITY = \"quality_agent\"\n",
    "    ORCHESTRATOR = \"orchestrator\"\n",
    "\n",
    "@dataclass\n",
    "class AgentMessage:\n",
    "    role: AgentRole\n",
    "    content: str\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class MultiAgentResponse:\n",
    "    question: str\n",
    "    final_answer: str\n",
    "    task_text: str\n",
    "    solution_text: str\n",
    "    conversation_history: List[AgentMessage]\n",
    "    citations: List[str]\n",
    "    avg_relevance: float  # NEW: retrieval quality from TopicAgent\n",
    "    iterations: int\n",
    "    quality_score: float\n",
    "    answer_length: int\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'question': self.question,\n",
    "            'final_answer': self.final_answer,\n",
    "            'task_text': self.task_text,\n",
    "            'solution_text': self.solution_text,\n",
    "            'avg_relevance': self.avg_relevance,\n",
    "            'iterations': self.iterations,\n",
    "            'quality_score': self.quality_score,\n",
    "            'answer_length': self.answer_length,\n",
    "            'num_messages': len(self.conversation_history)\n",
    "        }\n",
    "\n",
    "print(\"Dataclasses defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6210cc",
   "metadata": {
    "papermill": {
     "duration": 0.001528,
     "end_time": "2025-12-23T15:39:30.508612",
     "exception": false,
     "start_time": "2025-12-23T15:39:30.507084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Load Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dca9336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:30.511686Z",
     "iopub.status.busy": "2025-12-23T15:39:30.511596Z",
     "iopub.status.idle": "2025-12-23T15:39:43.726673Z",
     "shell.execute_reply": "2025-12-23T15:39:43.726267Z"
    },
    "papermill": {
     "duration": 13.217059,
     "end_time": "2025-12-23T15:39:43.726976",
     "exception": false,
     "start_time": "2025-12-23T15:39:30.509917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING INFRASTRUCTURE\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB: 15,836 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from '/home/sskaplun/study/genAI/kaggle/models/gemma-2-9b-it' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d84161b1a37466d8992ca0e07f3bba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOADING INFRASTRUCTURE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Vector DB\n",
    "client = chromadb.PersistentClient(path=str(DB_PATH))\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")\n",
    "collection = client.get_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "print(f\"Vector DB: {collection.count():,} chunks\")\n",
    "\n",
    "# LLM\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MODEL_PATH))\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    str(MODEL_PATH),\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "print(\"LLM loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a504457",
   "metadata": {
    "papermill": {
     "duration": 0.030941,
     "end_time": "2025-12-23T15:39:43.759911",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.728970",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Define Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd5025a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.763843Z",
     "iopub.status.busy": "2025-12-23T15:39:43.763458Z",
     "iopub.status.idle": "2025-12-23T15:39:43.766344Z",
     "shell.execute_reply": "2025-12-23T15:39:43.766003Z"
    },
    "papermill": {
     "duration": 0.005175,
     "end_time": "2025-12-23T15:39:43.766629",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.761454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core LLM function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_llm(prompt: str, max_tokens: int = MAX_NEW_TOKENS, temp: float = TEMPERATURE) -> str:\n",
    "    \"\"\"Core LLM generation function used by all agents.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(formatted, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temp,\n",
    "            top_p=0.9,\n",
    "            do_sample=temp > 0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    ).strip()\n",
    "\n",
    "print(\"Core LLM function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3accb9ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.770165Z",
     "iopub.status.busy": "2025-12-23T15:39:43.770066Z",
     "iopub.status.idle": "2025-12-23T15:39:43.773155Z",
     "shell.execute_reply": "2025-12-23T15:39:43.772822Z"
    },
    "papermill": {
     "duration": 0.005263,
     "end_time": "2025-12-23T15:39:43.773442",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.768179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopicAgent defined\n"
     ]
    }
   ],
   "source": [
    "class TopicAgent:\n",
    "    \"\"\"Agent 1: Retrieves relevant textbook context for a topic.\"\"\"\n",
    "    \n",
    "    def __init__(self, collection, k: int = TOP_K):\n",
    "        self.collection = collection\n",
    "        self.k = k\n",
    "        self.role = AgentRole.TOPIC\n",
    "    \n",
    "    def retrieve(self, topic: str) -> AgentMessage:\n",
    "        \"\"\"Retrieve context for topic.\"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_texts=[topic],\n",
    "            n_results=self.k\n",
    "        )\n",
    "        \n",
    "        chunks = []\n",
    "        citations = []\n",
    "        \n",
    "        for i, (doc, meta, dist) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            citation = f\"[{meta['filename']}, с. {meta['page_start']}-{meta['page_end']}]\"\n",
    "            header = f\"[Джерело {i}] {citation} | Тип: {meta['content_type']}\"\n",
    "            chunks.append(f\"{header}\\n{doc}\")\n",
    "            citations.append(citation)\n",
    "        \n",
    "        context = \"\\n\\n\".join(chunks)\n",
    "        avg_relevance = float(np.mean([1 - d for d in results['distances'][0]]))\n",
    "        \n",
    "        return AgentMessage(\n",
    "            role=self.role,\n",
    "            content=context,\n",
    "            metadata={\n",
    "                'citations': citations,\n",
    "                'avg_relevance': avg_relevance,\n",
    "                'num_chunks': len(chunks)\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"TopicAgent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4a7a06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.776981Z",
     "iopub.status.busy": "2025-12-23T15:39:43.776899Z",
     "iopub.status.idle": "2025-12-23T15:39:43.779156Z",
     "shell.execute_reply": "2025-12-23T15:39:43.778810Z"
    },
    "papermill": {
     "duration": 0.00432,
     "end_time": "2025-12-23T15:39:43.779347",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.775027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaskGeneratorAgent defined\n"
     ]
    }
   ],
   "source": [
    "class TaskGeneratorAgent:\n",
    "    \"\"\"Agent 2: Generates math problems from context.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.role = AgentRole.TASK_GENERATOR\n",
    "        self.system_prompt = \"\"\"Ти — експерт-агент з ГЕНЕРАЦІЇ математичних задач українською мовою.\n",
    "\n",
    "Твоя ЄДИНА задача: згенерувати ТІЛЬКИ текст задачі (умову) на основі контексту.\n",
    "\n",
    "Правила:\n",
    "- Використовуй ТІЛЬКИ інформацію з наданого контексту\n",
    "- Формулюй задачу чітко та зрозуміло\n",
    "- Включи конкретні числові значення\n",
    "- Використовуй українську математичну термінологію\n",
    "- НЕ пиши розв'язання (це зробить інший агент)\n",
    "\n",
    "Формат:\n",
    "**Задача:** [текст умови задачі]\n",
    "\n",
    "Все! Більше нічого не пиши.\"\"\"\n",
    "    \n",
    "    def generate(self, context: str, topic: str) -> AgentMessage:\n",
    "        \"\"\"Generate task from context.\"\"\"\n",
    "        prompt = f\"{self.system_prompt}\\n\\nКОНТЕКСТ:\\n{context}\\n\\nТЕМА: {topic}\\n\\nТВОЯ ЗАДАЧА:\"\n",
    "        task = generate_llm(prompt, max_tokens=300)\n",
    "        \n",
    "        return AgentMessage(\n",
    "            role=self.role,\n",
    "            content=task,\n",
    "            metadata={'topic': topic}\n",
    "        )\n",
    "\n",
    "print(\"TaskGeneratorAgent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c26590a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.782836Z",
     "iopub.status.busy": "2025-12-23T15:39:43.782750Z",
     "iopub.status.idle": "2025-12-23T15:39:43.784977Z",
     "shell.execute_reply": "2025-12-23T15:39:43.784670Z"
    },
    "papermill": {
     "duration": 0.004353,
     "end_time": "2025-12-23T15:39:43.785184",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.780831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SolutionAgent defined\n"
     ]
    }
   ],
   "source": [
    "class SolutionAgent:\n",
    "    \"\"\"Agent 3: Solves math problems step-by-step.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.role = AgentRole.SOLUTION\n",
    "        self.system_prompt = \"\"\"Ти — експерт-агент з РОЗВ'ЯЗУВАННЯ математичних задач українською мовою.\n",
    "\n",
    "Твоя ЄДИНА задача: надати покрокове розв'язання для заданої задачі.\n",
    "\n",
    "Правила:\n",
    "- Розв'язуй крок за кроком\n",
    "- Поясни кожен крок зрозумілою мовою\n",
    "- Використовуй формули з контексту\n",
    "- Перевіряй обчислення\n",
    "- Дай фінальну відповідь\n",
    "\n",
    "Формат:\n",
    "**Розв'язання:**\n",
    "1. [перший крок з поясненням]\n",
    "2. [другий крок]\n",
    "...\n",
    "\n",
    "**Відповідь:** [фінальна відповідь]\"\"\"\n",
    "    \n",
    "    def solve(self, task: str, context: str) -> AgentMessage:\n",
    "        \"\"\"Solve the task.\"\"\"\n",
    "        prompt = f\"{self.system_prompt}\\n\\nКОНТЕКСТ (формули та теорія):\\n{context}\\n\\n{task}\\n\\nТВОЄ РОЗВ'ЯЗАННЯ:\"\n",
    "        solution = generate_llm(prompt, max_tokens=500)\n",
    "        \n",
    "        return AgentMessage(\n",
    "            role=self.role,\n",
    "            content=solution,\n",
    "            metadata={'task': task}\n",
    "        )\n",
    "\n",
    "print(\"SolutionAgent defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590c1d3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.788740Z",
     "iopub.status.busy": "2025-12-23T15:39:43.788646Z",
     "iopub.status.idle": "2025-12-23T15:39:43.791192Z",
     "shell.execute_reply": "2025-12-23T15:39:43.790924Z"
    },
    "papermill": {
     "duration": 0.004758,
     "end_time": "2025-12-23T15:39:43.791490",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.786732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QualityAgent defined\n"
     ]
    }
   ],
   "source": [
    "class QualityAgent:\n",
    "    \"\"\"Agent 4: Validates task and solution quality.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.role = AgentRole.QUALITY\n",
    "        self.system_prompt = \"\"\"Ти — експерт-агент з КОНТРОЛЮ ЯКОСТІ математичних задач та розв'язань.\n",
    "\n",
    "Твоя задача: оцінити якість задачі та розв'язання за критеріями:\n",
    "1. Чіткість формулювання задачі (0-1)\n",
    "2. Коректність розв'язання (0-1)\n",
    "3. Повнота пояснення (0-1)\n",
    "4. Українська мова (0-1)\n",
    "5. Відповідність контексту (0-1)\n",
    "\n",
    "Формат відповіді:\n",
    "ОЦІНКА: [число від 0.0 до 1.0]\n",
    "КОМЕНТАР: [короткий коментар]\n",
    "ПРОПОЗИЦІЇ: [що покращити, якщо оцінка < 0.7]\"\"\"\n",
    "    \n",
    "    def validate(self, task: str, solution: str, context: str) -> Tuple[float, str, AgentMessage]:\n",
    "        \"\"\"Validate quality and return score, feedback, and message.\"\"\"\n",
    "        prompt = f\"{self.system_prompt}\\n\\nКОНТЕКСТ:\\n{context}\\n\\n{task}\\n\\n{solution}\\n\\nТВОЯ ОЦІНКА:\"\n",
    "        feedback = generate_llm(prompt, max_tokens=200, temp=0.3)\n",
    "        \n",
    "        # Extract score (simple regex)\n",
    "        import re\n",
    "        score_match = re.search(r'ОЦІНКА:\\s*([0-9.]+)', feedback)\n",
    "        score = float(score_match.group(1)) if score_match else 0.5\n",
    "        score = max(0.0, min(1.0, score))  # Clamp to [0, 1]\n",
    "        \n",
    "        return score, feedback, AgentMessage(\n",
    "            role=self.role,\n",
    "            content=feedback,\n",
    "            metadata={'score': score}\n",
    "        )\n",
    "\n",
    "print(\"QualityAgent defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3278cd7",
   "metadata": {
    "papermill": {
     "duration": 0.001516,
     "end_time": "2025-12-23T15:39:43.794539",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.793023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7418803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.798296Z",
     "iopub.status.busy": "2025-12-23T15:39:43.798186Z",
     "iopub.status.idle": "2025-12-23T15:39:43.802784Z",
     "shell.execute_reply": "2025-12-23T15:39:43.802488Z"
    },
    "papermill": {
     "duration": 0.006941,
     "end_time": "2025-12-23T15:39:43.803073",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.796132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orchestrator defined\n"
     ]
    }
   ],
   "source": [
    "class Orchestrator:\n",
    "    \"\"\"Agent 5: Coordinates multi-agent collaboration.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        topic_agent: TopicAgent,\n",
    "        task_agent: TaskGeneratorAgent,\n",
    "        solution_agent: SolutionAgent,\n",
    "        quality_agent: QualityAgent,\n",
    "        quality_threshold: float = QUALITY_THRESHOLD,\n",
    "        max_iterations: int = MAX_ITERATIONS\n",
    "    ):\n",
    "        self.topic_agent = topic_agent\n",
    "        self.task_agent = task_agent\n",
    "        self.solution_agent = solution_agent\n",
    "        self.quality_agent = quality_agent\n",
    "        self.quality_threshold = quality_threshold\n",
    "        self.max_iterations = max_iterations\n",
    "        self.role = AgentRole.ORCHESTRATOR\n",
    "    \n",
    "    def run(self, question: str, verbose: bool = False) -> MultiAgentResponse:\n",
    "        \"\"\"Orchestrate multi-agent workflow.\"\"\"\n",
    "        conversation = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n[ORCHESTRATOR] Starting workflow for: {question}\")\n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Step 1: Topic Agent retrieves context\n",
    "        if verbose:\n",
    "            print(\"\\n[1] TopicAgent: Retrieving context...\")\n",
    "        \n",
    "        topic_msg = self.topic_agent.retrieve(question)\n",
    "        conversation.append(topic_msg)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"    Retrieved {topic_msg.metadata['num_chunks']} chunks\")\n",
    "            print(f\"    Avg relevance: {topic_msg.metadata['avg_relevance']:.3f}\")\n",
    "        \n",
    "        context = topic_msg.content\n",
    "        citations = topic_msg.metadata['citations']\n",
    "        avg_relevance = topic_msg.metadata['avg_relevance']  # NEW: capture retrieval quality\n",
    "        \n",
    "        # Iterative refinement loop\n",
    "        for iteration in range(1, self.max_iterations + 1):\n",
    "            if verbose:\n",
    "                print(f\"\\n[ITERATION {iteration}]\")\n",
    "            \n",
    "            # Step 2: Task Generator creates problem\n",
    "            if verbose:\n",
    "                print(\"  [2] TaskGeneratorAgent: Creating task...\")\n",
    "            \n",
    "            task_msg = self.task_agent.generate(context, question)\n",
    "            conversation.append(task_msg)\n",
    "            task_text = task_msg.content\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"      Task: {task_text[:80]}...\")\n",
    "            \n",
    "            # Step 3: Solution Agent solves\n",
    "            if verbose:\n",
    "                print(\"  [3] SolutionAgent: Solving task...\")\n",
    "            \n",
    "            solution_msg = self.solution_agent.solve(task_text, context)\n",
    "            conversation.append(solution_msg)\n",
    "            solution_text = solution_msg.content\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"      Solution: {solution_text[:80]}...\")\n",
    "            \n",
    "            # Step 4: Quality Agent validates\n",
    "            if verbose:\n",
    "                print(\"  [4] QualityAgent: Validating...\")\n",
    "            \n",
    "            score, feedback, quality_msg = self.quality_agent.validate(\n",
    "                task_text, solution_text, context\n",
    "            )\n",
    "            conversation.append(quality_msg)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"      Quality Score: {score:.3f}\")\n",
    "                print(f\"      Threshold: {self.quality_threshold}\")\n",
    "            \n",
    "            # Check if quality is acceptable\n",
    "            if score >= self.quality_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"\\n[ORCHESTRATOR] Quality acceptable. Completing workflow.\")\n",
    "                break\n",
    "            elif iteration < self.max_iterations:\n",
    "                if verbose:\n",
    "                    print(f\"\\n[ORCHESTRATOR] Quality below threshold. Refining...\")\n",
    "                # In a real system, we'd use feedback to guide refinement\n",
    "                # For simplicity, we just retry\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"\\n[ORCHESTRATOR] Max iterations reached. Accepting current result.\")\n",
    "        \n",
    "        # Combine final answer\n",
    "        final_answer = f\"{task_text}\\n\\n{solution_text}\"\n",
    "        \n",
    "        return MultiAgentResponse(\n",
    "            question=question,\n",
    "            final_answer=final_answer,\n",
    "            task_text=task_text,\n",
    "            solution_text=solution_text,\n",
    "            conversation_history=conversation,\n",
    "            citations=citations,\n",
    "            avg_relevance=avg_relevance,  # NEW: pass retrieval quality\n",
    "            iterations=iteration,\n",
    "            quality_score=score,\n",
    "            answer_length=len(final_answer)\n",
    "        )\n",
    "\n",
    "print(\"Orchestrator defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e38fb",
   "metadata": {
    "papermill": {
     "duration": 0.001541,
     "end_time": "2025-12-23T15:39:43.806184",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.804643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Initialize Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7753869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.809856Z",
     "iopub.status.busy": "2025-12-23T15:39:43.809788Z",
     "iopub.status.idle": "2025-12-23T15:39:43.811960Z",
     "shell.execute_reply": "2025-12-23T15:39:43.811696Z"
    },
    "papermill": {
     "duration": 0.004401,
     "end_time": "2025-12-23T15:39:43.812202",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.807801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING MULTI-AGENT SYSTEM\n",
      "================================================================================\n",
      "Agents initialized:\n",
      "  - TopicAgent (RAG)\n",
      "  - TaskGeneratorAgent\n",
      "  - SolutionAgent\n",
      "  - QualityAgent\n",
      "  - Orchestrator\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING MULTI-AGENT SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create agents\n",
    "topic_agent = TopicAgent(collection, k=TOP_K)\n",
    "task_agent = TaskGeneratorAgent()\n",
    "solution_agent = SolutionAgent()\n",
    "quality_agent = QualityAgent()\n",
    "\n",
    "# Create orchestrator\n",
    "orchestrator = Orchestrator(\n",
    "    topic_agent=topic_agent,\n",
    "    task_agent=task_agent,\n",
    "    solution_agent=solution_agent,\n",
    "    quality_agent=quality_agent,\n",
    "    quality_threshold=QUALITY_THRESHOLD,\n",
    "    max_iterations=MAX_ITERATIONS\n",
    ")\n",
    "\n",
    "print(\"Agents initialized:\")\n",
    "print(\"  - TopicAgent (RAG)\")\n",
    "print(\"  - TaskGeneratorAgent\")\n",
    "print(\"  - SolutionAgent\")\n",
    "print(\"  - QualityAgent\")\n",
    "print(\"  - Orchestrator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77862cc3",
   "metadata": {
    "papermill": {
     "duration": 0.001605,
     "end_time": "2025-12-23T15:39:43.815421",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.813816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Test Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518b7547",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.819242Z",
     "iopub.status.busy": "2025-12-23T15:39:43.819162Z",
     "iopub.status.idle": "2025-12-23T15:39:43.821716Z",
     "shell.execute_reply": "2025-12-23T15:39:43.821391Z"
    },
    "papermill": {
     "duration": 0.00479,
     "end_time": "2025-12-23T15:39:43.821947",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.817157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: 5 questions\n",
      "Expected answers loaded for 10 questions\n"
     ]
    }
   ],
   "source": [
    "from common import STANDARD_TEST_QUESTIONS, EVALUATION_DATASET\n",
    "\n",
    "TEST_QUESTIONS = STANDARD_TEST_QUESTIONS[:5]  # Use first 5 questions for multi-agent\n",
    "print(f\"Test set: {len(TEST_QUESTIONS)} questions\")\n",
    "\n",
    "# Create mapping of questions to expected answers\n",
    "question_to_expected = {q['input']: q['expected_answer'] for q in EVALUATION_DATASET}\n",
    "print(f\"Expected answers loaded for {len(question_to_expected)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e043213",
   "metadata": {
    "papermill": {
     "duration": 0.001612,
     "end_time": "2025-12-23T15:39:43.825267",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.823655",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Run Multi-Agent Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf0da21f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:39:43.829119Z",
     "iopub.status.busy": "2025-12-23T15:39:43.829030Z",
     "iopub.status.idle": "2025-12-23T15:41:58.158088Z",
     "shell.execute_reply": "2025-12-23T15:41:58.157666Z"
    },
    "papermill": {
     "duration": 134.333104,
     "end_time": "2025-12-23T15:41:58.160066",
     "exception": false,
     "start_time": "2025-12-23T15:39:43.826962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING MULTI-AGENT EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUESTION 1/5: Квадратні рівняння\n",
      "================================================================================\n",
      "\n",
      "[ORCHESTRATOR] Starting workflow for: Квадратні рівняння\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] TopicAgent: Retrieving context...\n",
      "    Retrieved 5 chunks\n",
      "    Avg relevance: 0.785\n",
      "\n",
      "[ITERATION 1]\n",
      "  [2] TaskGeneratorAgent: Creating task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Task: **Задача:**  Знайти сторони двох квадратів, якщо сума їх площ дорівнює 208, а су...\n",
      "  [3] SolutionAgent: Solving task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Solution: **Розв'язання:**\n",
      "\n",
      "1. **Нехай:**\n",
      "   *  x - сторона першого квадрата\n",
      "   *  y - сто...\n",
      "  [4] QualityAgent: Validating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Quality Score: 0.900\n",
      "      Threshold: 0.7\n",
      "\n",
      "[ORCHESTRATOR] Quality acceptable. Completing workflow.\n",
      "\n",
      "[FINAL RESULT]\n",
      "--------------------------------------------------------------------------------\n",
      "**Задача:**  Знайти сторони двох квадратів, якщо сума їх площ дорівнює 208, а сума їх сторін дорівнює 20.\n",
      "\n",
      "**Розв'язання:**\n",
      "\n",
      "1. **Нехай:**\n",
      "   *  x - сторона першого квадрата\n",
      "   *  y - сторона другого квадрата\n",
      "\n",
      "2. **Запишемо рівняння згідно з умовою задачі:**\n",
      "   *  x² + y² = 208  (сума площ квадратів)\n",
      "   *  x + y = 20 (сума сторін квадратів)\n",
      "\n",
      "3. **Виразимо x з другого рівняння:**\n",
      "   * x = 20 - y\n",
      "\n",
      "4. **Підставимо цю вираз для x в перше рівняння:**\n",
      "   * (20 - y)² + y² = 208\n",
      "\n",
      "5. **Розкриємо квадрат та спростимо рівняння:**\n",
      "   * 400 - 40y + y² + y² = 208\n",
      "   * 2y² - 40y + 192 = 0\n",
      "\n",
      "6. **Розв'яжемо отримане рівняння за допомогою формули для квадратного рівняння:**\n",
      "   * y = (40 ± √(40² - 4 * 2 * 192)) / (2 * 2)\n",
      "   * y = (40 ± √(1600 - 1536)) / 4\n",
      "   * y = (40 ± √64) / 4\n",
      "   * y = (40 ± 8) / 4\n",
      "   * y₁ = 12, y₂ = 8\n",
      "\n",
      "7. **Підставимо значення y в рівняння x = 20 - y, щоб знайти x:**\n",
      "   * Для y₁ = 12: x = 20 - 12 = 8\n",
      "   * Для y₂ = 8: x = 20 - 8 = 12\n",
      "\n",
      "**Відповідь:** Сторони двох квадратів дорівнюють 8 і 12.\n",
      "--------------------------------------------------------------------------------\n",
      "Iterations: 1\n",
      "Quality Score: 0.900\n",
      "Citations: 5\n",
      "\n",
      "================================================================================\n",
      "QUESTION 2/5: Квадратні рівняння\n",
      "================================================================================\n",
      "\n",
      "[ORCHESTRATOR] Starting workflow for: Квадратні рівняння\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] TopicAgent: Retrieving context...\n",
      "    Retrieved 5 chunks\n",
      "    Avg relevance: 0.785\n",
      "\n",
      "[ITERATION 1]\n",
      "  [2] TaskGeneratorAgent: Creating task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Task: **Задача:** Знайдіть сторони двох квадратів, якщо сума їх площ дорівнює 208, а с...\n",
      "  [3] SolutionAgent: Solving task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Solution: **Розв'язання:**\n",
      "\n",
      "1. **Нехай x — сторона першого квадрата, а y — сторона другого...\n",
      "  [4] QualityAgent: Validating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Quality Score: 0.900\n",
      "      Threshold: 0.7\n",
      "\n",
      "[ORCHESTRATOR] Quality acceptable. Completing workflow.\n",
      "\n",
      "[FINAL RESULT]\n",
      "--------------------------------------------------------------------------------\n",
      "**Задача:** Знайдіть сторони двох квадратів, якщо сума їх площ дорівнює 208, а сума їх сторін дорівнює 20.\n",
      "\n",
      "**Розв'язання:**\n",
      "\n",
      "1. **Нехай x — сторона першого квадрата, а y — сторона другого квадрата.**\n",
      "\n",
      "2. **Запишемо систему рівнянь, що описує задачу:**\n",
      "   *  x² + y² = 208 (сума площ)\n",
      "   *  x + y = 20 (сума сторін)\n",
      "\n",
      "3. **Виразимо y з другого рівняння:**\n",
      "   *  y = 20 - x\n",
      "\n",
      "4. **Підставимо це значення y в перше рівняння:**\n",
      "   *  x² + (20 - x)² = 208\n",
      "\n",
      "5. **Розкриємо квадрат:**\n",
      "   *  x² + 400 - 40x + x² = 208\n",
      "\n",
      "6. **Упростимо рівняння:**\n",
      "   *  2x² - 40x + 192 = 0\n",
      "\n",
      "7. **Розділимо рівняння на 2:**\n",
      "   *  x² - 20x + 96 = 0\n",
      "\n",
      "8. **Знайдемо корені цього рівняння за формулою:**\n",
      "   *  x = (20 ± √(20² - 4 * 1 * 96)) / (2 * 1)\n",
      "   *  x = (20 ± √(400 - 384)) / 2\n",
      "   *  x = (20 ± √16) / 2\n",
      "   *  x = (20 ± 4) / 2\n",
      "\n",
      "9. **Знайдемо два можливі значення x:**\n",
      "   *  x₁ = (20 + 4) / 2 = 12\n",
      "   *  x₂ = (20 - 4) / 2 = 8\n",
      "\n",
      "10. **Знайдемо відповідні значення y:**\n",
      "   *  y₁ = 20 - x₁ = 20 - 12 = 8\n",
      "   *  y₂ = 20 - x₂ = 20 - 8 = 12\n",
      "\n",
      "**Відповідь:** Сторони двох квадратів дорівнюють 8 і 12.\n",
      "--------------------------------------------------------------------------------\n",
      "Iterations: 1\n",
      "Quality Score: 0.900\n",
      "Citations: 5\n",
      "\n",
      "================================================================================\n",
      "QUESTION 3/5: Квадратні рівняння\n",
      "================================================================================\n",
      "\n",
      "[ORCHESTRATOR] Starting workflow for: Квадратні рівняння\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] TopicAgent: Retrieving context...\n",
      "    Retrieved 5 chunks\n",
      "    Avg relevance: 0.785\n",
      "\n",
      "[ITERATION 1]\n",
      "  [2] TaskGeneratorAgent: Creating task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Task: **Задача:**  Знайти сторони двох квадратів, якщо їх сума дорівнює 20, а сума їх ...\n",
      "  [3] SolutionAgent: Solving task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Solution: **Розв'язання:**\n",
      "\n",
      "1. **Нехай x — сторона першого квадрата, а y — сторона другого...\n",
      "  [4] QualityAgent: Validating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Quality Score: 0.800\n",
      "      Threshold: 0.7\n",
      "\n",
      "[ORCHESTRATOR] Quality acceptable. Completing workflow.\n",
      "\n",
      "[FINAL RESULT]\n",
      "--------------------------------------------------------------------------------\n",
      "**Задача:**  Знайти сторони двох квадратів, якщо їх сума дорівнює 20, а сума їх квадратів дорівнює 208.\n",
      "\n",
      "**Розв'язання:**\n",
      "\n",
      "1. **Нехай x — сторона першого квадрата, а y — сторона другого квадрата.**\n",
      "\n",
      "2. **Запишемо задачу як систему рівнянь:**\n",
      "    *  x + y = 20  (сума сторін дорівнює 20)\n",
      "    *  x² + y² = 208 (сума квадратів сторін дорівнює 208)\n",
      "\n",
      "3. **Виразимо x з першого рівняння:**\n",
      "    * x = 20 - y\n",
      "\n",
      "4. **Підставимо цю вираз у друге рівняння:**\n",
      "    * (20 - y)² + y² = 208\n",
      "\n",
      "5. **Розкриємо квадрат:**\n",
      "    * 400 - 40y + y² + y² = 208\n",
      "\n",
      "6. **Згрупуємо подібні члени:**\n",
      "    * 2y² - 40y + 192 = 0\n",
      "\n",
      "7. **Розділимо рівняння на 2:**\n",
      "    * y² - 20y + 96 = 0\n",
      "\n",
      "8. **Знайдемо корені цього рівняння (можна за допомогою формули для коренів квадратного рівняння):**\n",
      "    * y = (20 ± √(20² - 4 * 1 * 96)) / (2 * 1)\n",
      "    * y = (20 ± √(400 - 384)) / 2\n",
      "    * y = (20 ± √16) / 2\n",
      "    * y = (20 ± 4) / 2\n",
      "\n",
      "9. **Отже, є два можливі значення для y:**\n",
      "    * y₁ = (20 + 4) / 2 = 12\n",
      "    * y₂ = (20 - 4) / 2 = 8\n",
      "\n",
      "10. **Підставимо кожне значення y у рівняння x = 20 - y, щоб знайти відповідні значення x:**\n",
      "    * Для y₁ = 12: x = 20 - 12 = 8\n",
      "    * Для y₂ = 8: x = 2\n",
      "--------------------------------------------------------------------------------\n",
      "Iterations: 1\n",
      "Quality Score: 0.800\n",
      "Citations: 5\n",
      "\n",
      "================================================================================\n",
      "QUESTION 4/5: Теорема Піфагора\n",
      "================================================================================\n",
      "\n",
      "[ORCHESTRATOR] Starting workflow for: Теорема Піфагора\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] TopicAgent: Retrieving context...\n",
      "    Retrieved 5 chunks\n",
      "    Avg relevance: 0.731\n",
      "\n",
      "[ITERATION 1]\n",
      "  [2] TaskGeneratorAgent: Creating task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Task: **Задача:**\n",
      "\n",
      "У прямокутному трикутнику ABC, де кут C прямий, довжина катета AC д...\n",
      "  [3] SolutionAgent: Solving task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Solution: **Розв'язання:**\n",
      "\n",
      "1. **Використовуємо теорему Піфагора:** У прямокутному трикутн...\n",
      "  [4] QualityAgent: Validating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Quality Score: 1.000\n",
      "      Threshold: 0.7\n",
      "\n",
      "[ORCHESTRATOR] Quality acceptable. Completing workflow.\n",
      "\n",
      "[FINAL RESULT]\n",
      "--------------------------------------------------------------------------------\n",
      "**Задача:**\n",
      "\n",
      "У прямокутному трикутнику ABC, де кут C прямий, довжина катета AC дорівнює 12 см, а довжина гіпотенузи AB дорівнює 15 см. Знайдіть довжину другого катета BC.\n",
      "\n",
      "**Розв'язання:**\n",
      "\n",
      "1. **Використовуємо теорему Піфагора:** У прямокутному трикутнику квадрат гіпотенузи дорівнює сумі квадратів катетів. \n",
      "\n",
      "2. **Запишемо формулу:** AB² = AC² + BC²\n",
      "\n",
      "3. **Підставляємо відомі значення:** 15² = 12² + BC²\n",
      "\n",
      "4. **Розраховуємо:** 225 = 144 + BC²\n",
      "\n",
      "5. **Знаходимо BC²:** BC² = 225 - 144 = 81\n",
      "\n",
      "6. **Знаходимо BC:** BC = √81 = 9 см\n",
      "\n",
      "**Відповідь:** Довжина другого катета BC дорівнює 9 см.\n",
      "--------------------------------------------------------------------------------\n",
      "Iterations: 1\n",
      "Quality Score: 1.000\n",
      "Citations: 5\n",
      "\n",
      "================================================================================\n",
      "QUESTION 5/5: Теорема Піфагора\n",
      "================================================================================\n",
      "\n",
      "[ORCHESTRATOR] Starting workflow for: Теорема Піфагора\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[1] TopicAgent: Retrieving context...\n",
      "    Retrieved 5 chunks\n",
      "    Avg relevance: 0.731\n",
      "\n",
      "[ITERATION 1]\n",
      "  [2] TaskGeneratorAgent: Creating task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Task: **Задача:**\n",
      "\n",
      "У прямокутному трикутнику ABC з прямим кутом при вершині C, довжина...\n",
      "  [3] SolutionAgent: Solving task...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Solution: **Розв'язання:**\n",
      "\n",
      "1. **Ідентифікуємо відомі дані:**\n",
      "    -  В прямокутному трикут...\n",
      "  [4] QualityAgent: Validating...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Quality Score: 0.900\n",
      "      Threshold: 0.7\n",
      "\n",
      "[ORCHESTRATOR] Quality acceptable. Completing workflow.\n",
      "\n",
      "[FINAL RESULT]\n",
      "--------------------------------------------------------------------------------\n",
      "**Задача:**\n",
      "\n",
      "У прямокутному трикутнику ABC з прямим кутом при вершині C, довжина катета AC дорівнює 5 см, а гіпотенуза AB = 13 см. Знайдіть довжину катета BC.\n",
      "\n",
      "**Розв'язання:**\n",
      "\n",
      "1. **Ідентифікуємо відомі дані:**\n",
      "    -  В прямокутному трикутнику ABC, прямий кут при вершині C.\n",
      "    - AC = 5 см (довжина одного катета)\n",
      "    - AB = 13 см (довжина гіпотенузи)\n",
      "\n",
      "2. **Виберемо формулу:**\n",
      "    -  За теоремою Піфагора:  AB² = AC² + BC²\n",
      "\n",
      "3. **Підставимо відомі значення:**\n",
      "    - 13² = 5² + BC²\n",
      "\n",
      "4. **Розв'яжемо рівняння:**\n",
      "    - 169 = 25 + BC²\n",
      "    - BC² = 169 - 25\n",
      "    - BC² = 144\n",
      "    - BC = √144 \n",
      "    - BC = 12 см\n",
      "\n",
      "**Відповідь:** Довжина катета BC дорівнює 12 см.\n",
      "--------------------------------------------------------------------------------\n",
      "Iterations: 1\n",
      "Quality Score: 0.900\n",
      "Citations: 5\n",
      "\n",
      "================================================================================\n",
      "Completed 5 multi-agent workflows\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RUNNING MULTI-AGENT EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "responses = []\n",
    "\n",
    "for i, question in enumerate(TEST_QUESTIONS, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"QUESTION {i}/{len(TEST_QUESTIONS)}: {question}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    response = orchestrator.run(question, verbose=True)\n",
    "    responses.append(response)\n",
    "    \n",
    "    print(f\"\\n[FINAL RESULT]\")\n",
    "    print(\"-\"*80)\n",
    "    print(response.final_answer)\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Iterations: {response.iterations}\")\n",
    "    print(f\"Quality Score: {response.quality_score:.3f}\")\n",
    "    print(f\"Citations: {len(response.citations)}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Completed {len(responses)} multi-agent workflows\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c18ae",
   "metadata": {
    "papermill": {
     "duration": 0.002064,
     "end_time": "2025-12-23T15:41:58.164369",
     "exception": false,
     "start_time": "2025-12-23T15:41:58.162305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d500d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:41:58.168996Z",
     "iopub.status.busy": "2025-12-23T15:41:58.168909Z",
     "iopub.status.idle": "2025-12-23T15:41:58.170752Z",
     "shell.execute_reply": "2025-12-23T15:41:58.170330Z"
    },
    "papermill": {
     "duration": 0.004617,
     "end_time": "2025-12-23T15:41:58.171013",
     "exception": false,
     "start_time": "2025-12-23T15:41:58.166396",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions loaded from common.py\n"
     ]
    }
   ],
   "source": [
    "import common\n",
    "\n",
    "print(\"Evaluation functions loaded from common.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dbd60b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:41:58.175850Z",
     "iopub.status.busy": "2025-12-23T15:41:58.175748Z",
     "iopub.status.idle": "2025-12-23T15:41:58.180057Z",
     "shell.execute_reply": "2025-12-23T15:41:58.179755Z"
    },
    "papermill": {
     "duration": 0.00704,
     "end_time": "2025-12-23T15:41:58.180332",
     "exception": false,
     "start_time": "2025-12-23T15:41:58.173292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EVALUATION\n",
      "================================================================================\n",
      "\n",
      "1. Квадратні рівняння...\n",
      "   Overall: 0.936 | Quality: 0.900 | Iterations: 1\n",
      "\n",
      "2. Квадратні рівняння...\n",
      "   Overall: 0.936 | Quality: 0.900 | Iterations: 1\n",
      "\n",
      "3. Квадратні рівняння...\n",
      "   Overall: 0.822 | Quality: 0.800 | Iterations: 1\n",
      "\n",
      "4. Теорема Піфагора...\n",
      "   Overall: 0.789 | Quality: 1.000 | Iterations: 1\n",
      "\n",
      "5. Теорема Піфагора...\n",
      "   Overall: 0.769 | Quality: 0.900 | Iterations: 1\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "  overall_score            : 0.851\n",
      "  quality_score            : 0.900\n",
      "  retrieval_quality        : 0.763\n",
      "  ukrainian_ratio          : 0.924\n",
      "  completeness             : 1.000\n",
      "  correctness              : 0.600\n",
      "  structure_rate           : 0.800\n",
      "  citation_rate            : 0.200\n",
      "  collaboration_quality    : 0.500\n",
      "  avg_iterations           : 1.000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluations = []\n",
    "\n",
    "for i, response in enumerate(responses, 1):\n",
    "    expected_answer = question_to_expected.get(response.question, None)\n",
    "    metrics = common.evaluate_multi_agent(\n",
    "        response.final_answer, \n",
    "        response.answer_length, \n",
    "        response.avg_relevance, \n",
    "        response.quality_score, \n",
    "        response.iterations,\n",
    "        expected_answer\n",
    "    )\n",
    "    evaluations.append({\n",
    "        'question': response.question,\n",
    "        'metrics': metrics,\n",
    "        'answer_length': response.answer_length,\n",
    "        'iterations': response.iterations,\n",
    "        'quality_score': response.quality_score\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{i}. {response.question[:50]}...\")\n",
    "    print(f\"   Overall: {metrics['overall_score']:.3f} | \"\n",
    "          f\"Quality: {metrics['quality_score']:.3f} | \"\n",
    "          f\"Iterations: {response.iterations}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_metrics = {\n",
    "    'overall_score': np.mean([e['metrics']['overall_score'] for e in evaluations]),\n",
    "    'quality_score': np.mean([e['metrics']['quality_score'] for e in evaluations]),\n",
    "    'retrieval_quality': np.mean([e['metrics']['retrieval_quality'] for e in evaluations]),\n",
    "    'ukrainian_ratio': np.mean([e['metrics']['ukrainian_ratio'] for e in evaluations]),\n",
    "    'completeness': np.mean([e['metrics']['completeness'] for e in evaluations]),\n",
    "    'correctness': np.mean([e['metrics']['correctness'] for e in evaluations]),\n",
    "    'structure_rate': sum(e['metrics']['has_structure'] for e in evaluations) / len(evaluations),\n",
    "    'citation_rate': sum(e['metrics']['has_citations'] for e in evaluations) / len(evaluations),\n",
    "    'collaboration_quality': np.mean([e['metrics']['collaboration_quality'] for e in evaluations]),\n",
    "    'avg_iterations': np.mean([e['iterations'] for e in evaluations])\n",
    "}\n",
    "\n",
    "for key, value in avg_metrics.items():\n",
    "    print(f\"  {key:25s}: {value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50728f22",
   "metadata": {
    "papermill": {
     "duration": 0.002021,
     "end_time": "2025-12-23T15:41:58.184515",
     "exception": false,
     "start_time": "2025-12-23T15:41:58.182494",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed28454b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-23T15:41:58.189173Z",
     "iopub.status.busy": "2025-12-23T15:41:58.189091Z",
     "iopub.status.idle": "2025-12-23T15:41:58.192190Z",
     "shell.execute_reply": "2025-12-23T15:41:58.191898Z"
    },
    "papermill": {
     "duration": 0.005811,
     "end_time": "2025-12-23T15:41:58.192445",
     "exception": false,
     "start_time": "2025-12-23T15:41:58.186634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ../evaluation/experiment_05\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 5 COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Overall Score: 0.851\n",
      "Quality Score: 0.900\n",
      "Avg Iterations: 1.0\n",
      "Collaboration Quality: 0.500\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    'experiment': 'multi_agent_system',\n",
    "    'description': 'Specialized agents with orchestration and quality validation',\n",
    "    'architecture': {\n",
    "        'agents': [\n",
    "            'TopicAgent (RAG)',\n",
    "            'TaskGeneratorAgent',\n",
    "            'SolutionAgent',\n",
    "            'QualityAgent',\n",
    "            'Orchestrator'\n",
    "        ],\n",
    "        'workflow': 'Retrieve → Generate Task → Solve → Validate → Iterate if needed',\n",
    "        'max_iterations': MAX_ITERATIONS,\n",
    "        'quality_threshold': QUALITY_THRESHOLD\n",
    "    },\n",
    "    'avg_metrics': avg_metrics,\n",
    "    'responses': [r.to_dict() for r in responses],\n",
    "    'evaluations': evaluations\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'results.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Results saved to {OUTPUT_DIR}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT 5 COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOverall Score: {avg_metrics['overall_score']:.3f}\")\n",
    "print(f\"Quality Score: {avg_metrics['quality_score']:.3f}\")\n",
    "print(f\"Avg Iterations: {avg_metrics['avg_iterations']:.1f}\")\n",
    "print(f\"Collaboration Quality: {avg_metrics['collaboration_quality']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 154.438535,
   "end_time": "2025-12-23T15:42:00.024374",
   "environment_variables": {},
   "exception": null,
   "input_path": "experiment_05_multi_agent.ipynb",
   "output_path": "results/run_20251223_172724/experiment_05_executed.ipynb",
   "parameters": {},
   "start_time": "2025-12-23T15:39:25.585839",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "53d911cf02194c91816ae174b430c941": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b062635630e143bcab42a6465ec43342",
       "placeholder": "​",
       "style": "IPY_MODEL_9adcbb57adbb41fa82a690586cd14702",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "7eed00faceda4ac6b94bc24635aba23b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8d84161b1a37466d8992ca0e07f3bba4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_53d911cf02194c91816ae174b430c941",
        "IPY_MODEL_adba7cfd9ce341a5ac961af0feb16426",
        "IPY_MODEL_99ff9d36ea3c46fd9db1bd2a478631f5"
       ],
       "layout": "IPY_MODEL_91dff21920d84ebf923b3b4dc18a417a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "91dff21920d84ebf923b3b4dc18a417a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99ff9d36ea3c46fd9db1bd2a478631f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a5dea3b203d24783beff416b6ed60ad1",
       "placeholder": "​",
       "style": "IPY_MODEL_7eed00faceda4ac6b94bc24635aba23b",
       "tabbable": null,
       "tooltip": null,
       "value": " 4/4 [00:08&lt;00:00,  2.06s/it]"
      }
     },
     "9adcbb57adbb41fa82a690586cd14702": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a5dea3b203d24783beff416b6ed60ad1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "adba7cfd9ce341a5ac961af0feb16426": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d914bcacb8ee4baba9b6664eaeae863e",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d8b4f4df77da40e58ce48f00468b75c9",
       "tabbable": null,
       "tooltip": null,
       "value": 4.0
      }
     },
     "b062635630e143bcab42a6465ec43342": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8b4f4df77da40e58ce48f00468b75c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d914bcacb8ee4baba9b6664eaeae863e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}